[{"id":0,"href":"/VisualComputing-Showcase/workshops/workshop1/workshop_1/","title":"Creating a New Theme","section":"Workshop1","content":" Introduction # This tutorial will show you how to create a simple theme in Hugo. I assume that you are familiar with HTML, the bash command line, and that you are comfortable using Markdown to format content. I\u0026rsquo;ll explain how Hugo uses templates and how you can organize your templates to create a theme. I won\u0026rsquo;t cover using CSS to style your theme.\nWe\u0026rsquo;ll start with creating a new site with a very basic template. Then we\u0026rsquo;ll add in a few pages and posts. With small variations on that, you will be able to create many different types of web sites.\nIn this tutorial, commands that you enter will start with the \u0026ldquo;$\u0026rdquo; prompt. The output will follow. Lines that start with \u0026ldquo;#\u0026rdquo; are comments that I\u0026rsquo;ve added to explain a point. When I show updates to a file, the \u0026ldquo;:wq\u0026rdquo; on the last line means to save the file.\nHere\u0026rsquo;s an example:\n## this is a comment $ echo this is a command this is a command ## edit the file $ vi foo.md +++ date = \u0026#34;2014-09-28\u0026#34; title = \u0026#34;creating a new theme\u0026#34; +++ bah and humbug :wq ## show it $ cat foo.md +++ date = \u0026#34;2014-09-28\u0026#34; title = \u0026#34;creating a new theme\u0026#34; +++ bah and humbug $ Some Definitions # There are a few concepts that you need to understand before creating a theme.\nSkins # Skins are the files responsible for the look and feel of your site. It’s the CSS that controls colors and fonts, it’s the Javascript that determines actions and reactions. It’s also the rules that Hugo uses to transform your content into the HTML that the site will serve to visitors.\nYou have two ways to create a skin. The simplest way is to create it in the layouts/ directory. If you do, then you don’t have to worry about configuring Hugo to recognize it. The first place that Hugo will look for rules and files is in the layouts/ directory so it will always find the skin.\nYour second choice is to create it in a sub-directory of the themes/ directory. If you do, then you must always tell Hugo where to search for the skin. It’s extra work, though, so why bother with it?\nThe difference between creating a skin in layouts/ and creating it in themes/ is very subtle. A skin in layouts/ can’t be customized without updating the templates and static files that it is built from. A skin created in themes/, on the other hand, can be and that makes it easier for other people to use it.\nThe rest of this tutorial will call a skin created in the themes/ directory a theme.\nNote that you can use this tutorial to create a skin in the layouts/ directory if you wish to. The main difference will be that you won’t need to update the site’s configuration file to use a theme.\nThe Home Page # The home page, or landing page, is the first page that many visitors to a site see. It is the index.html file in the root directory of the web site. Since Hugo writes files to the public/ directory, our home page is public/index.html.\n"},{"id":1,"href":"/VisualComputing-Showcase/workshops/workshop_1/","title":"Creating a New Theme","section":"Workshops","content":" Introduction # This tutorial will show you how to create a simple theme in Hugo. I assume that you are familiar with HTML, the bash command line, and that you are comfortable using Markdown to format content. I\u0026rsquo;ll explain how Hugo uses templates and how you can organize your templates to create a theme. I won\u0026rsquo;t cover using CSS to style your theme.\nWe\u0026rsquo;ll start with creating a new site with a very basic template. Then we\u0026rsquo;ll add in a few pages and posts. With small variations on that, you will be able to create many different types of web sites.\nIn this tutorial, commands that you enter will start with the \u0026ldquo;$\u0026rdquo; prompt. The output will follow. Lines that start with \u0026ldquo;#\u0026rdquo; are comments that I\u0026rsquo;ve added to explain a point. When I show updates to a file, the \u0026ldquo;:wq\u0026rdquo; on the last line means to save the file.\nHere\u0026rsquo;s an example:\n## this is a comment $ echo this is a command this is a command ## edit the file $ vi foo.md +++ date = \u0026#34;2014-09-28\u0026#34; title = \u0026#34;creating a new theme\u0026#34; +++ bah and humbug :wq ## show it $ cat foo.md +++ date = \u0026#34;2014-09-28\u0026#34; title = \u0026#34;creating a new theme\u0026#34; +++ bah and humbug $ Some Definitions # There are a few concepts that you need to understand before creating a theme.\nSkins # Skins are the files responsible for the look and feel of your site. It’s the CSS that controls colors and fonts, it’s the Javascript that determines actions and reactions. It’s also the rules that Hugo uses to transform your content into the HTML that the site will serve to visitors.\nYou have two ways to create a skin. The simplest way is to create it in the layouts/ directory. If you do, then you don’t have to worry about configuring Hugo to recognize it. The first place that Hugo will look for rules and files is in the layouts/ directory so it will always find the skin.\nYour second choice is to create it in a sub-directory of the themes/ directory. If you do, then you must always tell Hugo where to search for the skin. It’s extra work, though, so why bother with it?\nThe difference between creating a skin in layouts/ and creating it in themes/ is very subtle. A skin in layouts/ can’t be customized without updating the templates and static files that it is built from. A skin created in themes/, on the other hand, can be and that makes it easier for other people to use it.\nThe rest of this tutorial will call a skin created in the themes/ directory a theme.\nNote that you can use this tutorial to create a skin in the layouts/ directory if you wish to. The main difference will be that you won’t need to update the site’s configuration file to use a theme.\nThe Home Page # The home page, or landing page, is the first page that many visitors to a site see. It is the index.html file in the root directory of the web site. Since Hugo writes files to the public/ directory, our home page is public/index.html.\n"},{"id":2,"href":"/VisualComputing-Showcase/workshops/workshop3JL/","title":"Workshop3JL","section":"Workshops","content":" Shaders # Texturing and Coloring # I. Introducción # El coloreo y la texturización consiste en aplicar colores sobre formas o superficies mediante las coordenadas de los triángulos que las conforman. Esto nos permite ampliar nuestro abanico de posibilidades a la hora de darle apariencia a los objetos.\nII. Contextualización # Para este ejercicio, se empleó la técnica de mapeo de texturas y colores mediante las coordenadas UV. Estas son coordenadas que mapean las coordenadas de un objeto a la extensión del rango de valores de una fuente en dos coordenadas (U y V), de manera normalizada. Por ejemplo, si la fuente son los colores en formato RGB, usando dos de estas componentes, cada una con un rango de valores en el intervalo [0, 255], las coordenadas UV mapearán las coordenadas del objeto en cuestión a las coordenadas U (primera componente) y V (segunda componente) en un rango de [0, 1].\nSi la fuente fuera una textura, mediante una imagen, las coordenada U mapearia el rango de los pixeles de ancho [0, ancho] y la coordenada V haría lo correspondiente con los pixeles de alto [0, alto]. Luego, mediante el proceso de rasterización, se toman las coordenadas de los vértices del objeto en cuestión y mapeándolas a coordenadas UV se realiza la interpolación del color de los fragmentos (pixeles) de la textura de acuerdo a las coordenadas UV de la imagen.\nIII. Resultados # En este ejercicio se busca ilustrar un ejemplo del mapeo de dos de las componentes de color RGB a diferentes formas. Para la ilustración de cómo varía la coloración de cada componente a lo largo de la forma, se crearon tres formas: un cuadrado que se colorea con los componentes rojo - verde (RG), un círculo se colorea con verde - azul (GB) y un triángulo coloreado con rojo - azul (RB).\nLa implementación utilizando p5.js y el editor web se muestra a continuación:\nsketch.js let uvShader; function preload() { uvRGShader = readShader(\u0026#39;shaders/uv_rg.frag\u0026#39;, { matrices: Tree.pmvMatrix, varyings: Tree.texcoords2 }); uvRBShader = readShader(\u0026#39;shaders/uv_rb.frag\u0026#39;, { matrices: Tree.pmvMatrix, varyings: Tree.texcoords2 }); uvGBShader = readShader(\u0026#39;shaders/uv_gb.frag\u0026#39;, { matrices: Tree.pmvMatrix, varyings: Tree.texcoords2 }); } function setup() { createCanvas(300, 300, WEBGL); noStroke(); textureMode(NORMAL); } function draw() { background(0); shader(uvRGShader); quad(-width / 2, -height / 2, width / 2, -height / 2, width / 2, height / 2, -width / 2, height / 2); shader(uvRBShader); triangle(width / 2, height / 2, -width / 2, height / 2, 0 , 0); shader(uvGBShader); circle(0, -height / 4, width / 2, height / 2); } uv_gb.frag precision mediump float; varying vec2 texcoords2; void main() { gl_FragColor = vec4(0.0, texcoords2.xy, 1.0); } uv_rb.frag precision mediump float; varying vec2 texcoords2; void main() { gl_FragColor = vec4(texcoords2.x, 0.0, texcoords2.y, 1.0); } uv_rg.frag precision mediump float; varying vec2 texcoords2; void main() { gl_FragColor = vec4(texcoords2.yx, 0.0, 1.0); } Texture Sampling y Texture Tinting # I. Introducción # El filtrado o suavizado de texturas abarca los métodos empleados para reconstruir texturas mediante la modificación de los valores de los pixeles de estas de acuerdo a un criterio. El tinte de texturas consiste en operar los valores de las componentes RGB de los pixeles con información interpolada en estos, como la posición, la luz, y otro tipo de datos que se puedan asociar a los vértices de los triángulos que conforman el objeto en cuestión.\nII. Contextualización # En este ejercicio, se explora varias de las técnicas de filtrado de texturas, haciéndo énfasis en algunas de las coloring brightness tools, específicamente luma y component average.\nEstas técnicas nos permiten obtener información de las texturas que normalmente no es muy evitente en su representación de color RGB. Para el caso de luma, este filtrado consiste en calcular la componente con el mismo nombre, y se emplea para obtener información sobre la luminosidad de la textura.\nEl filtro component average lo que permite es obtener información respecto a la concentración de los componentes de color en cada pixel de una textura, pues se toma el valor de la componente R, G o B que sea más alto.\nPara la parte de tinte de texturas, se aplicó la siguiente lógica:\nSe reduce el valor de la componente roja en la medida que el pixel se encuentre más alejado a la izquierda (-x) del centro. Se reduce el valor de la componente verde en la medida que el pixel se encuentre más alejado hacia arriba (-y) del centro. Se reduce el valor de la componente azul en la medida que el pixel se encuentre más alejado hacia la derecha (+x) del centro. Se reduce el valor de las tres componentes en la medida que el pixel se encuentre más alejado hacia abajo (+y) del centro. III. Resultados # La implementación utilizando p5.js y el editor web realizada para lo expuesto anteriormente se muestra a continuación:\nsketch.js let lumaShader; let img; let grey_scale; let component_average; let texture_tinting; function preload() { lumaShader = readShader(\u0026#39;shaders/luma.frag\u0026#39;, { varyings: Tree.texcoords2 }); img = loadImage(\u0026#39;images/fire_breathing.jpg\u0026#39;); } function setup() { createCanvas(700, 500, WEBGL); noStroke(); textureMode(IMAGE); shader(lumaShader); grey_scale = createCheckbox(\u0026#39;luma\u0026#39;, false); grey_scale.position(10, 10); grey_scale.style(\u0026#39;color\u0026#39;, \u0026#39;white\u0026#39;); grey_scale.input(() =\u0026gt; lumaShader.setUniform(\u0026#39;grey_scale\u0026#39;, grey_scale.checked())); component_average = createCheckbox(\u0026#39;component average\u0026#39;, false); component_average.position(10, 30); component_average.style(\u0026#39;color\u0026#39;, \u0026#39;white\u0026#39;); component_average.input(() =\u0026gt; lumaShader.setUniform(\u0026#39;component_average\u0026#39;, component_average.checked())); texture_tinting = createCheckbox(\u0026#39;texture tinting\u0026#39;, false); texture_tinting.position(10, 50); texture_tinting.style(\u0026#39;color\u0026#39;, \u0026#39;white\u0026#39;); texture_tinting.input(() =\u0026gt; lumaShader.setUniform(\u0026#39;texture_tinting\u0026#39;, texture_tinting.checked())); lumaShader.setUniform(\u0026#39;texture\u0026#39;, img); } function draw() { background(0); quad(-width / 2, -height / 2, width / 2, -height / 2, width / 2, height / 2, -width / 2, height / 2); } .frag precision mediump float; uniform bool grey_scale; uniform bool component_average; uniform bool texture_tinting; uniform sampler2D texture; varying vec2 texcoords2; float luma(vec3 texel) { return 0.299 * texel.r + 0.587 * texel.g + 0.114 * texel.b; } float componentAverage(vec3 texel) { return 0.333 * texel.r + 0.333 * texel.g + 0.333 * texel.b; } vec3 textureTinting(vec3 texel, vec2 texcoords2) { float cut_point = 0.5; float red_modifier = texcoords2.x \u0026lt; cut_point ? texcoords2.x / cut_point : 1.0; float green_modifier = texcoords2.y \u0026lt; cut_point ? texcoords2.y / cut_point : 1.0; float blue_modifier = texcoords2.x \u0026gt; cut_point ? (texcoords2.x - cut_point) / cut_point : 1.0; float all_modifier = texcoords2.y \u0026gt; cut_point ? 1.0 - (texcoords2.y - cut_point) / cut_point : 1.0; return vec3( all_modifier * red_modifier * texel.r, all_modifier * green_modifier * texel.g, all_modifier * blue_modifier * texel.b ); } void main() { vec4 texel = texture2D(texture, texcoords2); if (grey_scale) { gl_FragColor = vec4((vec3(luma(texel.rgb))), 1.0); } else if (component_average) { gl_FragColor = vec4((vec3(componentAverage(texel.rgb))), 1.0); } else if (texture_tinting) { gl_FragColor = vec4(textureTinting(texel.rgb, texcoords2), 0.3); } else { gl_FragColor = texel; } } Image Processing # I. Introducción # El procesamiento de imágenes consiste en el empleo algoritmos para el procesamiento de imágenes digitales, con el multitud de fines: para mejorar la calidad de estas, aplicar efectos, obtener información importante, compresión, entre otros.\nII. Contextualización # En este ejercicio, el objetivo es experimentar con distintos algoritmos para el procesamiento de imágenes mediante la aplicación de efectos en una región específica (que en este caso se determina por la posición del ratón).\nEl primero de los efectos aplicados es el magnifier, que consiste en aplicar un efecto de ampliación de una región de la imagen, determinado por la resolución de la misma, y las dimensiones del \u0026ldquo;lente\u0026rdquo; o región sobre la cuál se aplica dicho efecto magnificador. Este efecto se adaptó del sitio web Shadertoy - Magnifier de wawan60\nEl segundo conjunto de efectos se realiza mediante la aplicación de un filtro mediante el uso de máscaras, las cuales consisten en matrices con las cuales se aplican modificaciones (filtros) a los vecinos de un pixel. Los efectos empleados fueron:\nBlur, que consigue dar un efecto \u0026ldquo;borroso\u0026rdquo; a la región de la imagen. Sharpen, que resalta detalles de los límites de un objeto a cambio de un aumento en el ruido de la imagen. Emboss, que aplica un resaltado o ensombrecimiento a los pixeles de una imagen, de acuerdo a los contornos claros / oscuros de esta. El tercer conjunto de efectos aplicado es el del filtrado o suavizado de texturas (presentado en el ejercicio anterior). Los efectos aplicados fueron:\nLuma, que toma la componente luma de luminosidad de cada pixel. HSV, que colorea los pixeles a partir de sus componentes de matiz, saturación y valor, calculados a partir de las componentes RGB. Value, que toma la componente de valor de HSV de cada pixel HSL, que colorea los pixeles a partir de sus componentes de matiz, saturación y luminosidad, calculados a partir de las componentes RGB. Ligthness, que toma la componente de luminosidad de HSL de cada pixel. III. Resultados # La implementación utilizando p5.js para los efectos anteriormente mencionados se muestra a continuación:\nsketch.js let effectShader; let img; let ridges; let blur; let sharpen; let emboss; let luma; let hsv; function preload() { effectShader = readShader(\u0026#39;shaders/effect.frag\u0026#39;, { varyings: Tree.texcoords2 }); img = loadImage(\u0026#39;images/one_piece.jpeg\u0026#39;); } function setup() { createCanvas(900, 450, WEBGL); noStroke(); textureMode(NORMAL); ridges = createCheckbox(\u0026#39;ridges\u0026#39;, false); ridges.position(10, 10); ridges.style(\u0026#39;color\u0026#39;, \u0026#39;white\u0026#39;); ridges.input(() =\u0026gt; effectShader.setUniform(\u0026#39;apply_convolution\u0026#39;, ridges.checked())); blur = createCheckbox(\u0026#39;blur\u0026#39;, false); blur.position(10, 30); blur.style(\u0026#39;color\u0026#39;, \u0026#39;white\u0026#39;); blur.input(() =\u0026gt; effectShader.setUniform(\u0026#39;apply_convolution\u0026#39;, blur.checked())); sharpen = createCheckbox(\u0026#39;sharpen\u0026#39;, false); sharpen.position(10, 50); sharpen.style(\u0026#39;color\u0026#39;, \u0026#39;white\u0026#39;); sharpen.input(() =\u0026gt; effectShader.setUniform(\u0026#39;apply_convolution\u0026#39;, sharpen.checked())); emboss = createCheckbox(\u0026#39;emboss\u0026#39;, false); emboss.position(10, 70); emboss.style(\u0026#39;color\u0026#39;, \u0026#39;white\u0026#39;); emboss.input(() =\u0026gt; effectShader.setUniform(\u0026#39;apply_convolution\u0026#39;, emboss.checked())); luma = createCheckbox(\u0026#39;luma\u0026#39;, false); luma.position(10, 90); luma.style(\u0026#39;color\u0026#39;, \u0026#39;white\u0026#39;); luma.input(() =\u0026gt; effectShader.setUniform(\u0026#39;apply_luma\u0026#39;, luma.checked())); hsv = createCheckbox(\u0026#39;hsv\u0026#39;, false); hsv.position(10, 110); hsv.style(\u0026#39;color\u0026#39;, \u0026#39;white\u0026#39;); hsv.input(() =\u0026gt; effectShader.setUniform(\u0026#39;apply_hsv\u0026#39;, hsv.checked())); hsv_v = createCheckbox(\u0026#39;hsv_v\u0026#39;, false); hsv_v.position(10, 130); hsv_v.style(\u0026#39;color\u0026#39;, \u0026#39;white\u0026#39;); hsv_v.input(() =\u0026gt; effectShader.setUniform(\u0026#39;apply_hsv_v\u0026#39;, hsv_v.checked())); hsl = createCheckbox(\u0026#39;hsl\u0026#39;, false); hsl.position(10, 150); hsl.style(\u0026#39;color\u0026#39;, \u0026#39;white\u0026#39;); hsl.input(() =\u0026gt; effectShader.setUniform(\u0026#39;apply_hsl\u0026#39;, hsl.checked())); hsl_l = createCheckbox(\u0026#39;hsl_l\u0026#39;, false); hsl_l.position(10, 170); hsl_l.style(\u0026#39;color\u0026#39;, \u0026#39;white\u0026#39;); hsl_l.input(() =\u0026gt; effectShader.setUniform(\u0026#39;apply_hsl_l\u0026#39;, hsl_l.checked())); shader(effectShader); effectShader.setUniform(\u0026#39;texture\u0026#39;, img); emitTexOffset(effectShader, img, \u0026#39;tex_offset\u0026#39;); } function draw() { background(0); if (ridges.checked()) { effectShader.setUniform(\u0026#39;mask\u0026#39;, [-1, -1, -1, -1, 8, -1, -1, -1, -1]); } else if (blur.checked()) { effectShader.setUniform(\u0026#39;mask\u0026#39;, [1/9, 1/9, 1/9, 1/9, 1/9, 1/9, 1/9, 1/9, 1/9]); } else if (sharpen.checked()) { effectShader.setUniform(\u0026#39;mask\u0026#39;, [0, -1, 0, -1, 5, -1, 0, -1, 0]); } else if (emboss.checked()) { effectShader.setUniform(\u0026#39;mask\u0026#39;, [-2, -1, 0, -1, 1, 1, 0, 1, 2]); } else if (luma.checked() || hsv.checked() || hsv_v.checked() || hsl.checked() || hsl_l.checked()) { effectShader.setUniform(\u0026#39;mask\u0026#39;, [0, 0, 0, 0, 1, 0, 0, 0, 0]); } else { effectShader.setUniform(\u0026#39;mask\u0026#39;, [0, 0, 0, 0, 1, 0, 0, 0, 0]); } const mx = map(mouseX, 0, width, 0.0, 1.0); const my = map(mouseY, 0, height, 0.0, 1.0); effectShader.setUniform(\u0026#39;u_mouse\u0026#39;, [mx, my]); effectShader.setUniform(\u0026#39;u_resolution\u0026#39;, [width, height]); quad(-width / 2, -height / 2, width / 2, -height / 2, width / 2, height / 2, -width / 2, height / 2); } effect.frag precision mediump float; uniform sampler2D texture; uniform vec2 tex_offset; uniform vec2 u_mouse; uniform vec2 u_resolution; uniform float mask[9]; uniform bool apply_convolution; uniform bool apply_luma; uniform bool apply_hsv; uniform bool apply_hsv_v; uniform bool apply_hsl; uniform bool apply_hsl_l; varying vec2 texcoords2; const float radius = 1.0; const float depth = radius / 2.0; float luma(vec3 texel) { return 0.299 * texel.r + 0.587 * texel.g + 0.114 * texel.b; } vec3 hsv(vec3 texel) { vec4 K = vec4(0.0, -1.0 / 3.0, 2.0 / 3.0, -1.0); vec4 p = mix(vec4(texel.bg, K.wz), vec4(texel.gb, K.xy), step(texel.b, texel.g)); vec4 q = mix(vec4(p.xyw, texel.r), vec4(texel.r, p.yzx), step(p.x, texel.r)); float d = q.x - min(q.w, q.y); float e = 1.0e-10; return vec3(abs(q.z + (q.w - q.y) / (6.0 * d + e)), d / (q.x + e), q.x); } float hsvV(vec3 texel) { return max(max(texel.r, texel.g), texel.b); } vec3 hsl(vec3 texel){ float h = 0.0; float s = 0.0; float l = 0.0; float r = texel.r; float g = texel.g; float b = texel.b; float c_min = min( r, min( g, b ) ); float c_max = max( r, max( g, b ) ); l = (c_max + c_min) / 2.0; if (c_max \u0026gt; c_min) { float c_delta = c_max - c_min; s = l \u0026lt; .0 ? c_delta / (c_max + c_min) : c_delta / (2.0 - (c_max + c_min)); if (r == c_max) { h = (g - b) / c_delta; } else if (g == c_max) { h = 2.0 + (b - r) / c_delta; } else { h = 4.0 + (r - g) / c_delta; } if (h \u0026lt; 0.0) { h += 6.0; } h = h / 6.0; } return vec3(h, s, l); } float hslL(vec3 texel) { return 0.21 * texel.r + 0.72 * texel.g + 0.07 * texel.b; } vec4 magnify(vec2 frag_coord) { vec2 uv = frag_coord / (u_resolution.xy * radius); vec2 center = vec2(u_mouse.x, 1.0 - u_mouse.y); float diff_x_sq = (uv.x - center.x) * (uv.x - center.x); float diff_y_sq = (uv.y - center.y) * (uv.y - center.y); float ax = diff_x_sq / (0.2 * 0.2) + (diff_y_sq / 0.1) * (u_resolution.x / u_resolution.y); float ax_sq = ax * ax; float radius_sq = radius * radius; float dx = (-depth / radius) * ax + (depth / radius_sq) * ax_sq; float f = ax + dx; if (ax \u0026gt; radius) f = ax; vec2 magnifier_area = center + (uv - center) * f / ax; vec2 magnifier_r = vec2(magnifier_area.x, 1.0 - magnifier_area.y); return texture2D(texture, magnifier_r); } vec4 convolute() { vec2 tc0 = texcoords2 + vec2(-tex_offset.s, -tex_offset.t); vec2 tc1 = texcoords2 + vec2(0.0, -tex_offset.t); vec2 tc2 = texcoords2 + vec2(tex_offset.s, -tex_offset.t); vec2 tc3 = texcoords2 + vec2(-tex_offset.s, 0.0); vec2 tc4 = texcoords2 + vec2(0.0, 0.0); vec2 tc5 = texcoords2 + vec2(tex_offset.s, 0.0); vec2 tc6 = texcoords2 + vec2(-tex_offset.s, +tex_offset.t); vec2 tc7 = texcoords2 + vec2(0.0, tex_offset.t); vec2 tc8 = texcoords2 + vec2(tex_offset.s, tex_offset.t); vec4 rgba[9]; rgba[0] = texture2D(texture, tc0); rgba[1] = texture2D(texture, tc1); rgba[2] = texture2D(texture, tc2); rgba[3] = texture2D(texture, tc3); rgba[4] = texture2D(texture, tc4); rgba[5] = texture2D(texture, tc5); rgba[6] = texture2D(texture, tc6); rgba[7] = texture2D(texture, tc7); rgba[8] = texture2D(texture, tc8); vec4 convolution; for (int i = 0; i \u0026lt; 9; ++i) { convolution += rgba[i] * mask[i]; } return convolution; } void main() { vec4 texel = texture2D(texture, texcoords2); vec2 uv = gl_FragCoord.xy / (u_resolution.xy * radius); vec2 center = vec2(u_mouse.x, 1.0 - u_mouse.y); float effect_region_normalized_radius = 0.1; bool is_in_effect_region = distance(uv, center) \u0026lt; effect_region_normalized_radius; if (is_in_effect_region) { if (apply_luma) { gl_FragColor = vec4(vec3(luma(texel.rgb)), 1.0); } else if (apply_hsv) { gl_FragColor = vec4(hsv(texel.rgb), 1.0); } else if (apply_hsv_v) { gl_FragColor = vec4(vec3(hsvV(texel.rgb)), 1.0); } else if (apply_hsl) { gl_FragColor = vec4(hsl(texel.rgb), 1.0); } else if (apply_hsl_l) { gl_FragColor = vec4(vec3(hslL(texel.rgb)), 1.0); } else if (apply_convolution) { vec4 convolution = convolute(); gl_FragColor = vec4(convolution.rgb, 1.0); } else { vec4 color = magnify(gl_FragCoord.xy); gl_FragColor = vec4(color.rgb, 1.0); } } else { gl_FragColor = vec4(texel.rgb, 1.0); } } Procedural Texturing # I. Introducción # Las texturas procedurales son aquellas que se construyen algorítmicamente en lugar de usar datos predefinidos como una imagen, las cuales pueden mapearse a objetos como cualquier otra textura y permiten agregar detalles complejos y dinámicos a objetos. Son muy empleadas para generar texturas de elementos naturales como la madera, el metal y la piedra.\nII. Contextualización # Para este ejercicio, se adaptaron tres shaders que implementan patrones generados mediante texturas procedurales de The Book of Shaders, de Patricio González y Jen Lowe.\nEl primer patrón, llamado Truchet Tiles, consiste en la disposición de parejas de elementos geométricos de dos colores diferentes intercalados con el objetivo de formar patrones complejos, y que mediante un algoritmo se mapee como textura procedural a una superficie (en este ejemplo, un toroide con forma de ovni). Al emplear esto junto con la interactividad del ratón, se puede reducir el tamaño y la cantidad de estos elementos geométricos y generar recursivamente estructuras fractales.\nEl segundo patrón consiste en aplicar el mapeo de color mediante dos componentes rgb (visto en el ejercicio 1) a los cuadrados que conforman una malla, la cual se mapea a una superficie. En este caso, se empleó el filtro HSV a los colores interpolados de los pixeles.\nEl último patrón es un ejemplo de patrones de \u0026ldquo;Offset\u0026rdquo;, aquellos patrones que intercalan la configuraciones de los objetos geométricos que los componen de acuerdo a una regla preestablecida. En este ejemplo, se ilustra el patrón de un muro de ladrillos, donde de acuerdo a la paridad del número de la fila, la posición de los rectángulos (ladrillos) posee una compensación con respecto a las filas de distinta paridad.\nAdemás, en este último ejercicio, se usó el filtro HSL para colorear los rectángulos, y adicionalmente, cuando se mueve el ratón disminuye el tamaño de estos (y por lo tanto la cantidad necesaria para cubrir la superficie cilíndrica) y cada segundo se cambia una de las componentes de color (antes de aplicar el filtro HSL).\nIII. Resultados # La implementación utilizando p5.js para los ejemplos anteriores se muestra a continuación:\nsketch.js let pg; let proceduralTexturingShader; function preload() { proceduralTexturingShader = readShader(\u0026#39;shaders/procedural_tex.frag\u0026#39;, { matrices: Tree.NONE, varyings: Tree.NONE }); } function setup() { createCanvas(400, 400, WEBGL); pg = createGraphics(400, 400, WEBGL); textureMode(NORMAL); noStroke(); pg.noStroke(); pg.textureMode(NORMAL); pg.shader(proceduralTexturingShader); pg.emitResolution(proceduralTexturingShader); proceduralTexturingShader.setUniform(\u0026#39;u_zoom\u0026#39;, 3); pg.quad(-1, -1, 1, -1, 1, 1, -1, 1); texture(pg); } function draw() { background(33); orbitControl(); torus(90, 65, 24, 4); } function mouseMoved() { proceduralTexturingShader.setUniform(\u0026#39;u_zoom\u0026#39;, int(map(mouseX, 0, width, 1, 30))); pg.quad(-1, -1, 1, -1, 1, 1, -1, 1); return false; } procedural_tex.frag precision mediump float; #define PI 3.14159265358979323846 uniform vec2 u_resolution; uniform float u_time; uniform float u_zoom; vec2 rotate2D (vec2 _st, float _angle) { _st -= 0.5; _st = mat2(cos(_angle), -sin(_angle), sin(_angle), cos(_angle)) * _st; _st += 0.5; return _st; } vec2 tile (vec2 _st, float _zoom) { _st *= _zoom; return fract(_st); } vec2 rotateTilePattern(vec2 _st){ _st *= 2.0; float index = 0.0; index += step(1., mod(_st.x, 2.0)); index += step(1., mod(_st.y, 2.0)) * 2.0; _st = fract(_st); // Rotate each cell according to the index if(index == 1.0){ // Rotate cell 1 by 180 degrees _st = rotate2D(_st, PI); } else if(index == 2.0){ // Rotate cell 2 by 90 degrees _st = rotate2D(_st, PI * 0.5); } else if(index == 3.0){ // Rotate cell 3 by -90 degrees _st = rotate2D(_st, PI * -0.5); } return _st; } void main (void) { vec2 st = gl_FragCoord.xy / u_resolution.xy; st = tile(st, u_zoom); st = rotate2D(st, -PI * u_time * 0.25); st = rotateTilePattern(st * 2.0); st = rotate2D(st, PI * u_time * 0.25); st = rotateTilePattern(st); gl_FragColor = vec4(vec3(step(st.x, st.y), 0.0, 1.0), 1.0); } sketch.js let pg; let proceduralTexturingShader; function preload() { proceduralTexturingShader = readShader(\u0026#39;shaders/procedural_tex.frag\u0026#39;, { matrices: Tree.NONE, varyings: Tree.NONE }); } function setup() { createCanvas(400, 400, WEBGL); pg = createGraphics(400, 400, WEBGL); textureMode(NORMAL); noStroke(); pg.noStroke(); pg.textureMode(NORMAL); pg.shader(proceduralTexturingShader); pg.emitResolution(proceduralTexturingShader); proceduralTexturingShader.setUniform(\u0026#39;u_zoom\u0026#39;, 3); pg.quad(-1, -1, 1, -1, 1, 1, -1, 1); texture(pg); } function draw() { background(33); orbitControl(); cone(120, 250, 24, 24); } function mouseMoved() { proceduralTexturingShader.setUniform(\u0026#39;u_zoom\u0026#39;, int(map(mouseX, 0, width, 1, 30))); pg.quad(-1, -1, 1, -1, 1, 1, -1, 1); return false; } procedural_tex.frag precision mediump float; uniform vec2 u_resolution; uniform float u_time; uniform float u_zoom; float circle(in vec2 _st, in float _radius){ vec2 l = _st - vec2(0.5); return 1. - smoothstep(_radius - _radius * 0.2, _radius + _radius * 0.2, dot(l, l) * 8.0); } vec3 hsv(vec3 texel) { vec4 K = vec4(0.0, -1.0 / 3.0, 2.0 / 3.0, -1.0); vec4 p = mix(vec4(texel.bg, K.wz), vec4(texel.gb, K.xy), step(texel.b, texel.g)); vec4 q = mix(vec4(p.xyw, texel.r), vec4(texel.r, p.yzx), step(p.x, texel.r)); float d = q.x - min(q.w, q.y); float e = 1.0e-10; return vec3(abs(q.z + (q.w - q.y) / (6.0 * d + e)), d / (q.x + e), q.x); } void main (void) { vec2 st = gl_FragCoord.xy / u_resolution; vec3 color = vec3(0.0); st *= u_zoom; st = fract(st); color = vec3(st, 1.0); gl_FragColor = vec4(hsv(color.rgb), 1.0); } sketch.js let pg; let proceduralTexturingShader; function preload() { proceduralTexturingShader = readShader(\u0026#39;shaders/procedural_tex.frag\u0026#39;, { matrices: Tree.NONE, varyings: Tree.NONE }); } function setup() { createCanvas(400, 400, WEBGL); pg = createGraphics(400, 400, WEBGL); textureMode(NORMAL); noStroke(); pg.noStroke(); pg.textureMode(NORMAL); pg.shader(proceduralTexturingShader); pg.emitResolution(proceduralTexturingShader); proceduralTexturingShader.setUniform(\u0026#39;u_zoom\u0026#39;, 3.0); pg.quad(-1, -1, 1, -1, 1, 1, -1, 1); texture(pg); } function draw() { background(33); orbitControl(); cylinder(80, 250, 25, 25); proceduralTexturingShader.setUniform(\u0026#39;u_seconds\u0026#39;, millis() / 1000.0); } function mouseMoved() { proceduralTexturingShader.setUniform(\u0026#39;u_zoom\u0026#39;, int(map(mouseX, 0, width, 1, 30))); pg.quad(-1, -1, 1, -1, 1, 1, -1, 1); return false; } procedural_tex.frag precision mediump float; uniform vec2 u_resolution; uniform float u_seconds; uniform float u_zoom; vec2 brickTile(vec2 _st, float _zoom){ _st *= _zoom; _st.x += step(1., mod(_st.y, 2.0)) * 0.5; return fract(_st); } vec3 hsl(vec3 texel){ float h = 0.0; float s = 0.0; float l = 0.0; float r = texel.r; float g = texel.g; float b = texel.b; float cMin = min( r, min( g, b ) ); float cMax = max( r, max( g, b ) ); l = (cMax + cMin) / 2.0; if (cMax \u0026gt; cMin) { float cDelta = cMax - cMin; s = l \u0026lt; .0 ? cDelta / (cMax + cMin) : cDelta / (2.0 - (cMax + cMin)); if (r == cMax) { h = (g - b) / cDelta; } else if (g == cMax) { h = 2.0 + (b - r) / cDelta; } else { h = 4.0 + (r - g) / cDelta; } if (h \u0026lt; 0.0) { h += 6.0; } h = h / 6.0; } return vec3(h, s, l); } float hslL(vec3 texel) { return 0.21 * texel.r + 0.72 * texel.g + 0.07 * texel.b; } float box(vec2 _st, vec2 _size){ _size = vec2(0.5) - _size * 0.5; vec2 uv = smoothstep(_size, _size + vec2(1e-4), _st); uv *= smoothstep(_size, _size + vec2(1e-4), vec2(1.0) - _st); return uv.x * uv.y; } void main(void){ vec2 st = gl_FragCoord.xy/u_resolution.xy; vec3 color = vec3(0.0); st = brickTile(st, u_zoom); float mod3 = u_seconds - (3.0 * floor(u_seconds / 3.0)); if (floor(mod3) == 0.0) color = vec3(st, 0.3); else if (floor(mod3) == 1.0) color = vec3(0.5, st); else if (floor(mod3) == 2.0) color = vec3(st, 0.5); gl_FragColor = vec4(vec3(hsl(color.rgb)), 1.0); } IV. Conclusiones # Los shaders son herramientas muy poderosas y esenciales en todo desarrollo moderno en el área de la computación gráficas, permitiendo el renderizado de objetos complejos de manera eficiente y versátil. A través de los ejercicios presentados en este taller, se ha obtenido un bagaje introductorio a los posibles usos de los shaders, de una forma incremental tanto en conceptos empleados, como en dificultad.\nV. Referencias # Wikipedia contributors. (2022, 23 junio). UV Mapping. Wikipedia. https://en.wikipedia.org/wiki/UV_mapping. Wikipedia Wikipedia contributors. (2022, 22 abril). HSL and HSV. Wikipedia. https://en.wikipedia.org/wiki/HSL_and_HSV. Wikipedia Shadertoy. (2022). Magnifier. https://www.shadertoy.com/view/llsSz7. Shadertoy EM. (2020). Sharpen Filter in Image Processing. GlobalSino. https://www.globalsino.com/EM/page1371.html GlobalSino The Book Of Shaders. (2015). Patterns. https://thebookofshaders.com/09/. The Book Of Shaders "},{"id":3,"href":"/VisualComputing-Showcase/workshops/workshop1/","title":"Workshop1","section":"Workshops","content":" Visual phenomena and Optical Illusions # I. Introducción # Las ilusiones ópticas son distorsiones de los sentidos causadas por el sistema visual, caracterizadas por una percepción que parece diferir de la realidad. Estas pueden revelar cómo el cerebro humano organiza e interpreta normalmente la estimulación sensorial y permiten estudiar las limitaciones de la percepción visual. Este tipo de \u0026ldquo;fenómenos\u0026rdquo; son adaptaciones especialmente buenas de nuestro sistema visual a situaciones de visión estándar; estas adaptaciones al depender de nuestro cerebro pueden provocar interpretaciones inadecuadas de la escena visual y dependen de la representación interna de la realidad una vez nuestros ojos han filtrado la información. Michael Bach es un científico alemán que ha investigado ampliamente el campo de la oftalmología y la percepción visual, y su sitio \u0026ldquo;Optical Illusions \u0026amp; Visual Phenomena\u0026rdquo; contiene un amplio repertorio de ilusiones ópticas que son claramente explicadas y su explicación nos acerca a entender un poco mejor de donde viene la interpretación que se da de estos fenómenos. Bach los clasifica en fenómenos de movimiento y tiempo, iluminación y contraste, color, ilusiones geométricas y angulares, entre otros. Para este trabajo se han seleccionado tres de los fenómenos/ilusiones propuestas por Bach, las cuales se analizarán en detalle y se presentará su correspondiente implementación en JavaScript utilizando la biblioteca de p5.js\nII. Contextualización # Stereokinetic Effect # El primer informe del fenómeno estereo cinético fue publicado en 1924 por Mussati, atribuyendo su descubrimiento a su maestro, Vittorio Benussi. Establecía que todas las percepciones de profundidad se basan en información de movimiento como fenómeno estereo cinético, y que entre los eventos que consideraba ejemplos de este efecto, estaban:\nPercibir la profundidad desde el paralaje de movimiento. Se refiere al hecho de que los objetos que se mueven a velocidad constante a lo largo del fotograma parecerán moverse en mayor proporción si se encuentran más cerca al observador. El efecto cinético de profundidad, que se refiere a la percepción tridimensional al observar las proyecciones de objetos rotando. Patrones bidimensionales que al rotar sobre el plano de una imagen evocan impresiones de profundidad. El patrón más comúnmente estudiado del efecto estéreo cinético, referido como SKE cone, consiste en patrones circulares anidados que rotan sobre una plataforma circular. Estos círculos, al rotar alrededor de algún eje distinto de la línea de visión de quien los observa, permiten extraer la configuración tridimensional del patrón, debido a varias transformaciones ópticas que se producen. Al hacer rotar varios círculos externos sobre el eje de la plataforma circular, y círculos internos sobre un círculo interno intermedio, en sentido contrario, se puede apreciar una ilusión de un objeto tridimensional cónico, y apreciar profundidad mediante los círculos internos [Figura 2]. Si la rotación se mantiene en un solo sentido para todos los círculos internos, se tiene la impresión de un cono que apunta hacia el observador, o visto desde otra perspectiva, el interior de un cono que apunta hacia el sentido opuesto al observador [Figura 1].\nFigura 1. Efecto estereo cinético de cono, donde todos los círculos internos rotan respecto a un círculo externo, dando la impresión de un cono apuntando hacia o en sentido contrario del observador.\nFigura 2. Efecto estereo cinético de cono, donde algunos los círculos internos rotan respecto a otro círculo interno, pudiendo obtener la percepción de profundidad de un cono abierto.\nCuando el movimiento a lo largo de los contornos de los círculos no es detectado, el patrón alcanza lo que Musatti llamó estabilidad de orientación, por lo parece que los contornos se mueven relativos a otros, lo cual a su vez evoca la percepción de una forma tridimensional, inclinada en profundidad en un ángulo consistente con su excentricidad y altura aparente y moviéndose alrededor de la línea de visión (el eje z) con componentes oscilatorios iguales en los ejes x e y.\nStroboscopic Artifacts # El segundo fenómeno visual consiste de una rueda o disco dividida en tres componentes que inicialmente corresponden cada una a un color primario del modelo RGB (rojo, verde y azul). La rueda gira en la dirección de las manecillas del reloj y a medida que incrementa el ángulo de rotación de ésta, al igual que el retardo entre actualizaciones dado en fotogramas se pueden observar interesantes cambios en las tonalidades de la misma, además de la \u0026ldquo;dirección\u0026rdquo; del movimiento. En este fenómeno, se pueden evidenciar momentos clave, que aparecen en ángulos de 60°, y más claramente 120°, donde la rueda toma valores similares al gris, debido a que cada sector alterna rápidamente entre los tres colores principales, cuya mezcla da como resultado el color evidenciado. Además, si se aumenta el ángulo 5°, parece surgir una \u0026ldquo;hélice\u0026rdquo; que gira hacia la derecha, donde cada pala está compuesta de los tres colores complementarios de los principales (magenta, cian y amarillo), por otro lado, si se disminuye en 5°, parece que la hélice gira hacia atrás.\nBach explica tal fenómeno desde la perspectiva del movimiento en las pantallas de ordenador, que cuando se trata de un movimiento rápido sufren del efecto estroboscópico o también denominado aliasing temporal. El aliasing se puede explicar bajo el escenario de que cuando se ve una imagen digital, un dispositivo de visualización, los ojos y el cerebro realizan una reconstrucción; si los datos de la imagen se procesan de alguna manera durante el muestreo o la reconstrucción, la imagen reconstruida diferirá de la imagen original, y se verá un alias que suele superponer al original (este efecto se evidencia comúnmente en el muestreo de señales). Por su parte, el efecto estroboscópico se produce cuando el movimiento rotativo continuo u otro movimiento cíclico se representa mediante una serie de muestras cortas o instantáneas (en contraposición a una vista continua) a una frecuencia de muestreo cercana al periodo del movimiento. Esta es la causa del \u0026ldquo;efecto rueda de carreta\u0026rdquo;, que Bach abarca también en su repertorio, llamado así porque en los videos, las ruedas (como las de los carros de caballos) a veces parecen girar hacia atrás. Para que se produzca tal efecto, es necesario que la pantalla se presente de forma discontinua: puede que no sea visible, pero la rueda (o lo que se presente en la pantalla) se mueve a \u0026ldquo;tirones\u0026rdquo;; si estas \u0026ldquo;sacudidas\u0026rdquo; se producen con suficiente rapidez (por ejemplo, 20 veces por segundo, o similar al caso de nuestro ejemplo), nuestro sistema visual interpola las posiciones que faltan. Esta interpolación se basa en el principio del \u0026ldquo;vecino más cercano\u0026rdquo; y si el desplazamiento del radio de la rueda de un fotograma a otro es tan grande que está más cerca del (antiguo) radio siguiente que del (antiguo) original, nuestro sistema visual asume la dirección de movimiento opuesta.\nBach también menciona la importancia de la frecuencia de fotogramas en el movimiento. El disco no gira suavemente, sino que se presenta en cuadros fijos que se suceden rápidamente, cada uno con un ángulo de rotación diferente, lo que produce la percepción de un movimiento suave, también llamado \u0026ldquo;Fenómeno Phi\u0026rdquo; de Wertheimer. Sin embargo, este movimiento depende en gran medida de la tasa de fotogramas, que para el caso predeterminado es de 60 cuadros mostrados por segundo y también entra en juego otro aspecto: la tasa de fotogramas del monitor o la pantalla. Según esto, entonces, lo que se puede apreciar exactamente depende de la interacción de las dos tasas de fotogramas y del incremento del ángulo, donde en variados casos según la configuración se puede evidenciar la \u0026ldquo;mezcla\u0026rdquo; de los colores, por el solapamiento de las secciones, además de los efectos ya mencionados.\nMoiré Patterns # Moiré es una palabra francesa que significa muaré en el español y es una textura o tipo de tejido que genera una visión sobre la seda simulando un entorno acuático y ondulado debido a la manera de su fabricación, que es la superposición de dos textiles húmedos generando un patrón cuando la seda se seca.\nEsta ilusión optica conocida como Moiré patterns o patrones muaré hace referencia a la superposición de dos patrones similares que están compuestos por rayas opacas ó de color junto con un espacio transaparente. Al encontrarse diferencias en los patrones, la colocación de los mismos, movimientos de rotación o desplazamiento y otros aspectos como aceleración o formatura, es posible generar una especie de bandas oscuras móviles conocidas como moirés. Este efecto no es simplemente aludido en el arte y la animación, sino que, tiene aplicaciones científicas en los campos de matemática y física en donde surgen cálculos con respecto a las formas, rotaciones, aceleraciones, desplazamientos, interferencia de ondas, entre otros más.\nFigura 4. Moirés generados por la rotación de uno de dos patrones de líneas paralelas negras. Imagen extraída de: Wikipedia\nLa explicación a detalle de esta ilusión consiste en un fenómeno relativo a la luz y la superposición, pues la luz impacta a ambos patrones que permiten que esta pueda pasar o quedar bloqueada según la forma y/o el color o transparencia en el que se encuentre impactando. Esto genera una multiplicación en las razones de transmitancia de luz y dos frecuencias que a la vista parecen distintas.\nFigura 5. Moirés generados el movimiento horizontal de un patron de círculos concéntricos.\nIII. Resultados # La implementación utilizando p5.js realizada para los casos anteriores se muestra a continuación:\np5-instance-div markdown const frame_rate = 60; let show_crater_cb; let show_crater = true; let slider_label; function setup() { createCanvas(500, 500); show_crater_cb = createCheckbox(\u0026#39;show crater\u0026#39;, show_crater); show_crater_cb.changed(() =\u0026gt; { show_crater = show_crater_cb.checked(); }); frames_slider = createSlider(0.5, 5, 1, 0.25); frames_slider.position(180, 515); frames_slider.style(\u0026#39;width\u0026#39;, \u0026#39;80px\u0026#39;); slider_label = createSpan(\u0026#39;speed\u0026#39;); slider_label.position(135, 513); frameRate(frame_rate); } function draw() { background(220); let difference = 40; let inner_diameter = 40; const outer_circles = 11; const start = inner_diameter + outer_circles * difference; const end = inner_diameter; noStroke(); let posX = 0, posY = 0; let referenceX = width / 2; let referenceY = height / 2; let t, outer_coeff, inner_coef; for (let diameter = start, index = 0; diameter \u0026gt;= end; diameter -= difference, index++) { fill(index % 2 === 0 ? color(\u0026#39;blue\u0026#39;) : color(\u0026#39;yellow\u0026#39;)); let orientation = index \u0026gt; 6 \u0026amp;\u0026amp; show_crater ? -1 : 1; outer_coeff = orientation * index * difference / 2; t = frameCount * frames_slider.value() / frame_rate; posX = referenceX + outer_coeff * cos(t); posY = referenceY + outer_coeff * sin(t); if (index == 6 \u0026amp;\u0026amp; show_crater) { inner_coef = diameter / 2 + difference; referenceX = posX + inner_coef * cos(t); referenceY = posY + inner_coef * sin(t); } circle(posX, posY, diameter); } } p5-instance-div markdown let angle = 0; let frames; let colorp1, colorp2, colorp3; let rotation_angle; function setup() { createCanvas(500, 500); frames_slider = createSlider(5, 120, 60, 5); frames_slider.position(180, 40); frames_slider.style(\u0026#39;width\u0026#39;, \u0026#39;80px\u0026#39;); ellipseMode(CENTER); rotation_angle = createP().position(25, 5); frames = createP().position(180, 5); slider = createSlider(0, 360, 0, 5); slider.position(20, 40); slider.style(\u0026#39;width\u0026#39;, \u0026#39;80px\u0026#39;); createP(\u0026#39;Colors:\u0026#39;).position(25, 55).style(\u0026#39;font-size: 15px\u0026#39;); colorp1 = createColorPicker([0, 255, 0]).position(20, 95); colorp2 = createColorPicker([0, 0, 255]).position(20, 135); colorp3 = createColorPicker([255, 0, 0]).position(20, 175); } function draw() { background(200); noStroke(); translate(width/2, height/2); rotate(angle); fill(colorp1.color()); arc(0, 0, width/2, height/2, 0, 2*PI/3); fill(colorp2.color()); arc(0, 0, width/2, height/2, 2*PI/3, 4*PI/3); fill(colorp3.color()); arc(0, 0, width/2, height/2, 4*PI/3, 2*PI); angle += radians(slider.value()); frameRate(frames_slider.value()); rotation_angle.html(\u0026#39;Rotation angle: \u0026#39; + slider.value()); frames.html(\u0026#39;Frame rate: \u0026#39; + frames_slider.value()); } p5-instance-div markdown let x = 0; let colorp1, colorp2; let increase = 0; function setup() { createCanvas(500, 500); rectMode(CENTER); colorp1 = createColorPicker([32, 162, 32]).position(20, 25); colorp2 = createColorPicker([0, 0, 255]).position(75, 25); slider = createSlider(0, 2, 0, 0.25); slider.position(150, 25); slider.style(\u0026#39;width\u0026#39;, \u0026#39;80px\u0026#39;); } function draw() { background(220); increase = slider.value(); for (let i = 0; i \u0026lt; 400; i += 20) { stroke(colorp2.color()); strokeWeight(4); ellipse(x, 250, i - 380, i - 380); noFill(); stroke(colorp1.color()); strokeWeight(4); ellipse(250, 250, i, i); } if (x \u0026gt; width) { x = 0; } else { x = x + increase; } } IV. Conclusiones # Para concluir, podemos resaltar en nuestras investigaciones que el campo de ilusiones ópticas es sumamente extenso y rico en información. Los límites en la generación de ilusiones son inexistentes y este informe presenta solo 3 tipos de ilusiones que pueden ser replicadas y visualizadas en una infinidad de experimentos distintos.\nComo trabajo futuro podemos resaltar aplicaciones interesantes de las anteriores ilusiones ópticas, como la impresión de imágenes con color, procesamiento y medición de deformaciones en imágenes y visualización de imágenes en pantallas y dispositivos electrónicos.\nIV. Referencias # Bach, M. (2022). 148 Visual Phenomena \u0026amp; Optical Illusions. Michael Bach Bach \u0026amp; Poloschek (2006) Optical Illusions Primer. Michael Bach Proffitt DR, Rock I, Hecht H, Schubert J. Stereokinetic effect and its relation to the kinetic depth effect. J Exp Psychol Hum Percept Perform. 1992 Feb;18(1):3-21. doi: 10.1037//0096-1523.18.1.3. PMID: 1532192. "},{"id":4,"href":"/VisualComputing-Showcase/workshops/workshop2/","title":"Workshop2","section":"Workshops","content":" Texture Mapping and Finger Tracking with ML5 MediaPipe Hands # Texture Mapping # I. Introducción # El mapeo de texturas es el método con el cual se realiza el detallado de color a un objeto 3D. Actualmente, se han desarrollado mapeos más complejos implicando diferentes tipos de transformaciones y polígonos como el mapeo de reflexión, relieve, entre otros.\nEl proceso consiste en la obtención de una imagen mediante fotografía digital o escaneo y su manipulación mediante software para ser utilizada. Posteriormente, la aplicación es el mapeo de vértices del polígono a una coordenada de textura. El mapeado UV consiste en el mapeado de un plano 2D a un tríangulo, ubicando los tres vertices del tríangulo en la imagen, con coordenadas normalizadas en la imagen y calculando los puntos interiores utilizando coordenadas báricentricas, se presentará su correspondiente implementación bajo nivel junto con mapeos de medio y alto nivel en JavaScript utilizando la biblioteca de p5.js y WEBGL.\nII. Contextualización # Mapeo de Alto Nivel # El primer mapeo de textura consiste en una esfera (sólido de revolución y no polígono) y una imagen de la tierra [figura 1] a alto nivel mediante el uso de las funciones texture() y sphere().\nFigura 1. Plano 2D de la tierra. Tomado de : Planet Texture Maps\nMapeo de Medio Nivel # El segundo mapeo de textura consiste en una esfera (sólido de revolución y no polígono) y una imagen de la tierra [figura 1] a medio nivel mediante el uso de las funciones texture(), beginShape() y endShape().\nPara realizar la esfera, utilizamos los conceptos de mapeo entre longitud/latitud en un plano 2D a la esfera utilizando ángulos.\nLos valores de longitud y latitud estarán:\nLongitud entre 0 y 360 grados. Latitud entre 0 y 180 grados. Buscamos mapear los valores de (r,lat,long) a (x,y,z).\nx = r sin(lat) cos(long) y = r sin(lat) sin(long) z = r cos(lat) p5-instance-div markdown const globe = []; const r = 200; const total = 25; let angleX = 0; let angleY = 0; function setup() { createCanvas(500, 500, WEBGL); noFill(); strokeWeight(2); stroke(200); for (let i = 0; i \u0026lt; total + 1; i++) { globe[i] = []; const lat = map(i, 0, total, 0, PI); for (let j = 0; j \u0026lt; total + 1; j++) { const lon = map(j, 0, total, 0, TWO_PI); const x = r * sin(lat) * cos(lon); const y = r * sin(lat) * sin(lon); const z = r * cos(lat); globe[i][j] = createVector(x, y, z); } } } function draw() { background(51); rotateX(angleX); rotateY(angleY); for (let i = 0; i \u0026lt; total; i++) { beginShape(TRIANGLE_STRIP); for (let j = 0; j \u0026lt; total + 1; j++) { const v1 = globe[i][j]; vertex(v1.x, v1.y, v1.z); const v2 = globe[i + 1][j]; vertex(v2.x, v2.y, v2.z); } endShape(); } angleX += 0.005; angleY += 0.006; } Figura 2. Esfera construida a bajo nivel.\nPor último, es necesario establecer el modo de textura normalizado y dividir nuestros valores de longitud (coordenada x) y latitud (coordenada y) entre el número total de vértices para darles un valor entre 0 y 1.\nMapeo de Bajo Nivel # El tercer mapeo de textura consiste en interpolar a bajo nivel los pixeles de una imagen de la tierra [figura 1] en un plano 2D con los triángulos que conforman el mesh de la elipse mediante el uso de las coordenadas baricéntricas, donde a partir de los (x, y) de tres vértices adyacentes, se calculan las coordenadas baricéntricas para los triángulos que conforman, y se interpola el valor RGBA de cada pixel. De esta manera se comprende mejor como las proporciones de la textura original se modifican de acuerdo a la figura que se busca texturizar.\nIII. Resultados # La implementación utilizando p5.js realizada para los casos anteriores se muestra a continuación:\np5-instance-div markdown let angleX = 0; let angleY = 0; function preload() { earth = loadImage( \u0026#34;https://lh6.googleusercontent.com/MKWuIXLwcIXgwmrKrnjgCFEjna_8kFePKfWJlhOQLpBZ3pagPVPjxyHxZPHs2CTGMm1sdKLx_WGkjVhnDF_L9EQbata6o2Cw0dtIvNYz-yQG_YJXNfpWff_HbdsNtqkWAia6jwG7aLWDbJbn6w\u0026#34; ); } function setup() { createCanvas(500, 500, WEBGL); noStroke(); } function draw() { background(51); rotateX(angleX); rotateY(angleY); textureMode(NORMAL); texture(earth); sphere(200); angleX += 0.005; angleY += 0.006; } Figura 3. Esfera mapeada con la imagen de la tierra a alto nivel.\np5-instance-div markdown const globe = []; const r = 200; const total = 50; let angleX = 0; let angleY = 0; function preload() { earth = loadImage( \u0026#34;https://lh6.googleusercontent.com/IwEkyWS6TCXKxJWlsIaylCZT53k3i6nhXs2xo6Fduap28MgLZMyypiK9KHvJDi7APkDkzh5-80y3i1PdPL_XeCn72HspV9z_jTThXpG3VCee0NUoJ_RBezRKSBWXn6YtgbBKPhL23x1ruQImzQ\u0026#34; ); } function setup() { createCanvas(500, 500, WEBGL); noFill(); noStroke(); for (let i = 0; i \u0026lt; total + 1; i++) { globe[i] = []; const lat = map(i, 0, total, 0, PI); for (let j = 0; j \u0026lt; total + 1; j++) { const lon = map(j, 0, total, 0, TWO_PI); const x = r * sin(lat) * cos(lon); const y = r * sin(lat) * sin(lon); const z = r * cos(lat); globe[i][j] = createVector(x, y, z); } } } function draw() { background(51); rotateX(angleX); rotateY(angleY); textureMode(NORMAL); texture(earth); scale(-1, 1); for (let i = 0; i \u0026lt; total; i++) { beginShape(TRIANGLE_STRIP); for (let j = 0; j \u0026lt; total + 1; j++) { const v1 = globe[i][j]; vertex(v1.x, v1.y, v1.z, j / total, i / total); const v2 = globe[i + 1][j]; vertex(v2.x, v2.y, v2.z, j / total, (i + 1) / total); } endShape(); } angleX += 0.005; angleY += 0.006; } Figura 4. Esfera mapeada con la imagen de la tierra a medio nivel.\np5-instance-div markdown const figure = []; const COLORS = 4; const ROWS = 500; const COLS = 1000; let earth; let triangleWidth; let triangleHeight; let matrix = []; let pg; function preload() { earth = loadImage(\u0026#39;https://lh6.googleusercontent.com/MKWuIXLwcIXgwmrKrnjgCFEjna_8kFePKfWJlhOQLpBZ3pagPVPjxyHxZPHs2CTGMm1sdKLx_WGkjVhnDF_L9EQbata6o2Cw0dtIvNYz-yQG_YJXNfpWff_HbdsNtqkWAia6jwG7aLWDbJbn6w\u0026#39;); //earth.loadPixels(); } function setup() { earth.loadPixels(); createCanvas(earth.width, earth.height); pixelDensity(1); pg = createGraphics(earth.width, earth.height); pg.pixelDensity(1); noLoop(); noFill(); strokeWeight(2); stroke(200); triangleWidth = earth.width / COLS; triangleHeight = earth.height / ROWS; for (let i = 0; i \u0026lt; earth.height; i++) { matrix.push([]); for (let j = 0; j \u0026lt; earth.width; j++) { matrix[i].push([]); for (let c = 0; c \u0026lt; COLORS; c++) { matrix[i][j][c] = c; } } } let index = 0, row = 0, col = 0; while (index \u0026lt; earth.pixels.length \u0026amp;\u0026amp; row \u0026lt; earth.height \u0026amp;\u0026amp; col \u0026lt; earth.width) { matrix[row][col][index % COLORS] = earth.pixels[index]; if ((col + 1) \u0026gt;= earth.width) { row++; col = 0; } else if (index % COLORS == 0) { col = (col + 1) % earth.width; } index++; } let minX = earth.height * 2; let maxX = 0; for (let i = 0; i \u0026lt; ROWS; i++) { figure[i] = []; for (let j = 0; j \u0026lt; COLS; j++) { const x = i + triangleHeight / 2; const y = j + triangleWidth / 2; minX = min(minX, x); maxX = max(maxX, x); figure[i][j] = createVector(x, y); } } print(minX); print(maxX); pg.loadPixels(); triangles(); pg.updatePixels(); image(pg, 0, 0, width, height); } function draw() { } function triangles() { for (let i = 0; i \u0026lt; ROWS - 1; i++) { for (let j = 0; j \u0026lt; COLS - 1; j++) { const v = figure[i][j]; const xVal = ((v.x - earth.height / 2) * (v.x - earth.height / 2)) / (earth.height * earth.height / 4); const yVal = ((v.y - earth.width / 2) * (v.y - earth.width / 2)) / (earth.width * earth.width / 4); if (xVal + yVal \u0026lt;= 1){ interpolateTrianglePixels(i, j); } } } } function interpolateTrianglePixels(row, col) { const row0 = row, col0 = col; const row1 = row + 1, col1 = col; const row2 = row, col2 = col + 1; const row3 = row + 1, col3 = col + 1; const v0 = figure[row0][col0]; const v1 = figure[row1][col1]; const v2 = figure[row2][col2]; const v3 = figure[row3][col3]; const row0Pix = floor(v0.x); const col0Pix = floor(v0.y); const row1Pix = floor(v1.x); const col1Pix = floor(v1.y); const row2Pix = floor(v2.x); const col2Pix = floor(v2.y); const row3Pix = floor(v3.x); const col3Pix = floor(v3.y); const r = 0, g = 1, b = 2, a = 3; const color0 = color( matrix[row0Pix][col0Pix][r], matrix[row0Pix][col0Pix][g], matrix[row0Pix][col0Pix][b], matrix[row0Pix][col0Pix][a] ); const color1 = color( matrix[row1Pix][col1Pix][r], matrix[row1Pix][col1Pix][g], matrix[row1Pix][col1Pix][b], matrix[row1Pix][col1Pix][a] ); const color2 = color( matrix[row2Pix][col2Pix][r], matrix[row2Pix][col2Pix][g], matrix[row2Pix][col2Pix][b], matrix[row2Pix][col2Pix][a] ); const color3 = color( matrix[row3Pix][col3Pix][r], matrix[row3Pix][col3Pix][g], matrix[row3Pix][col3Pix][b], matrix[row3Pix][col3Pix][a] ); for (let i = row0Pix; i \u0026lt; row3Pix; i++) { for (let j = col0Pix; j \u0026lt; col3Pix; j++) { let coordsT1 = barycentric_coords(i, j, row0Pix, col0Pix, row1Pix, col1Pix, row2Pix, col2Pix); let coordsT2 = barycentric_coords(i, j, row1Pix, col1Pix, row2Pix, col2Pix, row3Pix, col3Pix); setPixelColorValues(coordsT1, { color0, color1, color2, }, { i, j }); setPixelColorValues(coordsT2, { color0: color1, color1: color2, color2: color3, }, { i, j }); } } } function setPixelColorValues(coords, colors, pixelIndexes) { const { i, j } = pixelIndexes; const { color0, color1, color2 } = colors; const r = 0, g = 1, b = 2, a = 3; if (coords.w0 \u0026gt;= 0 \u0026amp;\u0026amp; coords.w1 \u0026gt;= 0 \u0026amp;\u0026amp; coords.w2 \u0026gt;= 0) { redVal = red(color0) + coords.w0 * (red(color1) - red(color0)) + coords.w1 * (red(color2) - red(color1)); greenVal = green(color0) + coords.w0 * (green(color1) - green(color0)) + coords.w1 * (green(color2) - green(color1)); blueVal = blue(color0) + coords.w0 * (blue(color1) - blue(color0)) + coords.w1 * (blue(color2) - blue(color1)); alphaVal = alpha(color0) + coords.w0 * (alpha(color1) - alpha(color0)) + coords.w1 * (alpha(color2) - alpha(color1)); pgIndex = (i * earth.width + j) * COLORS; pg.pixels[pgIndex + r] = redVal; pg.pixels[pgIndex + g] = greenVal; pg.pixels[pgIndex + b] = blueVal; pg.pixels[pgIndex + a] = alphaVal; } } function barycentric_coords(row, col, row0, col0, row1, col1, row2, col2) { let edges = edge_functions(row, col, row0, col0, row1, col1, row2, col2); let area = parallelogram_area(row0, col0, row1, col1, row2, col2); return { w0: edges.e12 / area, w1: edges.e20 / area, w2: edges.e01 / area }; } function parallelogram_area(row0, col0, row1, col1, row2, col2) { return (col1 - col0) * (row2 - row0) - (col2 - col0) * (row1 - row0); } function edge_functions(row, col, row0, col0, row1, col1, row2, col2) { let e01 = (row0 - row1) * col + (col1 - col0) * row + (col0 * row1 - row0 * col1); let e12 = (row1 - row2) * col + (col2 - col1) * row + (col1 * row2 - row1 * col2); let e20 = (row2 - row0) * col + (col0 - col2) * row + (col2 * row0 - row2 * col0); return { e01, e12, e20 }; } Figura 5. Mapeo de la textura al conjunto \u0026ldquo;aplanado\u0026rdquo; de triángulos que conforman la esfera\nIV. Conclusiones # Para concluir, podemos resaltar en nuestras investigaciones que el campo de mapeo de texturas ha sido apropiado para investigaciones y desarrollos más complejos, para entender este proceso es necesario entender las bases y el bajo nivel de los gráficos, ádemas de conceptos matemáticos claves como sistemas de coordenadas, geometría y otros involucrados en rasterización. Como trabajo futuro se puede proponer el mapeo de texturas de medio y bajo nivel a súper geometrías y el desarrollo de estas a nivel matemático debido al alto nivel de complejidad de estas figuras. Esta investigación se ha realizado de manera transversal por los distintos niveles de mapeo, afianzando los conocimientos y conceptos, convirtiéndose así en una buena ejemplificación práctica del mapeo de texturas.\nFinger Tracking with MediaPipe Hands # I. Introducción # MediaPipe es un framework gratis y de código abierto desarrollado por Google que tiene como objetivo la construcción de pipelines utilizando modelos de Inteligencia Artifical y Machine Learning listos para su uso, optimizados y para múltiples plataformas, fue presentado en el 2019 y a día de hoy sigue siendo mantenido y renovando por Google. Este Framework se basa en TensorFlow y TensorFlow Lite, que también son propiedad de Google, para el desarrollo de Machine Learning, redes neuronales y que cuentan con un alto prestigio en este sector. Actualmente MediaPipe es multiplataforma, ya que a pesar de ser desarollado en C++ cuenta con un API para python, Javascript y móviles. MediaPipe está fuertemente ligada al Processing y las soluciones gráficas ya que trabaja con procesos optimizados en la GPU y cuenta con distintos modelos previamente entrenados para la detección de caras, manos, objetos, poses en imágenes y videos, Box Tracking, Motion Tracking,entre otros, que son capaces de ejecutarse mediante p5.js.\nEn esta investigación se abarcará uno de los modelos en el campo del Finger Tracking denominado MediaPipe Hands, este se encuentra relacionado a gráficos, imagen y vídeo. También, se presentarán detalles del estado del arte e historia del campo del Finger Tracking, su funcionamiento, recolección de datos, cómo funciona a nivel teórico y cómo utilizar MediaPipe Hands en una aplicación de este modelo utilizando p5.js para generar un brush tridimensional.\nII. Contextualización # El Finger Tracking [Figura 5] es uno de los campos correspondientes al sector de interacción humano - máquina en 3 dimensiones, se empezó a estudiar en 1969 con los primeros reconocimientos de gestos en el procesamiento de imágenes. En este sector, una de las claves y el primer paso a seguir es la captura de datos del usuario, en el Finger Tracking como tal es posible capturar los datos utilizando o no una interfaz o hardware como lo son los guantes de captura, utilizar seguimientos de posición y reconstrucción espacial vía mandos, entre otros, estos métodos son conocidos como seguimientos de captura inercial y sus grandes desventajas son la interferencia magnética en los procesos y la dependencia del funcionamiento adecuado de los sensores.\nEl otro sistema es el basado en captura de la observación o movimiento óptico, este sistema utiliza marcadores que son puntos de interacción configurados previamente en los modelos con el fin de dar seguimiento a puntos estratégicos y la realización de cálculos espaciales necesarios en situaciones de óptica con condiciones adversas como el desenfoque, esto con el fin de mantener una congruencia espacial en la posición de los marcadores sin importar que se tengan algunas posiciones erradas o desconocidas. Támbien se utilizan patrones 3D entrenados previamente en los modelos para representar computacionalmente las coordenadas (incluyendo la profundidad) de los marcadores.\nFigura 5. Finger tracking y sus seguimientos. Tomado de : Microsoft\nPara el modelo utilizado en MediaPipe Hands de MediaPipe, se utiliza un seguimiento sin interfaz. Estos seguimientos están construidos con base en estimaciones de secuencias y un modelo de cambio con grados de libertad en las articulaciones conectadas como un cuerpo rígido. Se establece un espacio o grafo de estados con atributos de posición y ángulos para cada dedo funcional, esto con el fin de realizar una estimación. Los procesos comunes en este seguimiento son:\nSubstracción del fondo en la imagen. Convolución de la imagen a una máscara de binarios para segmentar la mano del fondo. Reconocimiento de regiones mediante la mano izquierda y derecha. Identificación de picos y valles de la mano, yemas de los dedos y transformación de estos datos a coordenadas 3D. Estimación de posición mediante el uso del modelo y el grafo de estados. Opcional: Reconocimiento de gestos. El modelo de estimación etiqueta cada articulación de los dedos, genera un sistema de ejes en cada articulación (21 en total) y proyecta un rayo utilizando la bisectriz. Con cada uno de estos rayos de proyección se estima a un punto en las 3 dimensiones y con estas correspondencias se estima la pose actual de la mano. El siguiente es un pseudocódigo que tiene en cuenta la oclusión de ciertas articulaciones:\n(a) Reconstruya los rayos de proyección a partir de los puntos de la imagen. (b) Para cada rayo de proyección R: (c) Para cada contorno 3D: (c1) Estime el punto P1 más cercano del rayo R a un punto en el contorno (c2) si (n == 1) elige P1 como P real para la correspondencia punto-línea (c3) si no, compare P1 con P: si dist(P1, R) es menor que dist(P, R) entonces elegir P1 como nuevo P (d) Utilice (P, R) como conjunto de correspondencia. (e) Estimar pose con este conjunto de correspondencia (f) Transformar contornos, ir a (b) En este campo también es posible resaltar la extracción de datos utilizando sistemas láser como el Smart Scanner Laser que es independiente de modelos para la conversión y estimación de 2D a 3D, ya que estos son capaces de obtener las coordenadas tridimensionales sin utilizar procesamiento de imágenes peor con un campo de visión más restringido.\nCon respecto a las aplicaciones actuales y desarrollos futuros se pueden resaltar:\nRealidad virtual. Realidad aumentada. Modelados 3D. Animación y Motion Capture. Simplificación de tareas computacionales vía gestos interactivos. Esta última aplicación junto con la realidad aumentada son con posibilidad las más amplias y ambiciosas pues pretenden facilitar en términos de usabilidad distintos sistemas computacionales mediante la interacción de gestos como el lenguaje de señas, esto sin perder precisión y rendimiento junto con la creación y masificación de experiencias con mayor inmersividad en sistemas que van desde sistemas de información y manejo de datos hasta juegos y filtros de la cámara. Empresas como Google [Figura 6], Apple, Microsoft trabajan diariamente en la mejora de precisión y rendimiento en sus sensores y modelos.\n_Figura 6. Lanzamiento del API de Google MediaPipe. Tomado de : Twitter*\nMediaPipe Hands utiliza este modelo de TensorFlow lanzado por Google en 2019 que le permite reconocer varias manos y 21 puntos de interacción en cada una sin utilizar entornos potentes de computación ya que, hasta un Smartphone puede ejecutarlo de manera adecuada. Utiliza un modelo de detección de palmas, basado en picos y valles junto con un modelo de detección de puntos de interacción sobre los resultados del modelo anterior, esto con el fin de proporcionar menos carga al segundo modelo y aumentar la precisión en la estimación de pose o coordenadas.\nEl API de Javascript cuenta con configuraciones opcionales como el número máximo de manos maxNumHands, la complejidad del modelo y los valores mínimos de confianza para las predicciones. La mejor manera de entender su uso es con un ejemplo práctico tridimensional, en este caso es la implementación de un brush.\nIII. Resultados # MediaPipe hands let detections = {}; const videoElement = document.getElementById(\u0026#34;video\u0026#34;); const hands = new Hands({ locateFile: (file) =\u0026gt; { return `https://cdn.jsdelivr.net/npm/@mediapipe/hands/${file}`; }, }); hands.setOptions({ maxNumHands: 2, modelComplexity: 1, minDetectionConfidence: 0.5, minTrackingConfidence: 0.5, }); hands.onResults(gotHands); function gotHands(results) { detections = results; console.log(detections); } const camera = new Camera(videoElement, { onFrame: async () =\u0026gt; { await hands.send({ image: videoElement }); }, width: 1280, height: 720, }); camera.start(); let video; let myHandpose; let myResults; // let color; let depth; let brush; let easycam; let state; let escorzo; let points; let record; let canvas; function setup() { canvas = createCanvas(980, 720, WEBGL); canvas.id(\u0026#34;canvas\u0026#34;); let state = { distance: 250, // scalar center: [0, 0, 0], // vector rotation: [0, 0, 0, 1], // quaternion }; easycam = createEasyCam(); easycam.state_reset = state; easycam.setState(state, 2000); escorzo = true; perspective(); points = []; depth = createSlider(0, 1, 0.05, 0.05); depth.position(10, 10); depth.style(\u0026#34;width\u0026#34;, \u0026#34;580px\u0026#34;); depth.hide(); // color = createColorPicker(\u0026#34;#000000\u0026#34;); // color.position(width - 70, 40); // // select initial brush brush = boxBrush; // push(); } function draw() { // pop(); if (detections) { if (detections.multiHandLandmarks) { calculateDistanceBetFingers(); update(); } } background(0); fill(166, 237, 19); sphere(10); push(); strokeWeight(0.8); stroke(\u0026#34;white\u0026#34;); grid({ dotted: false }); pop(); axes(); for (const point of points) { push(); translate(point.worldPosition); brush(point); pop(); } // push(); } function calculateDistanceBetFingers() { if (detections \u0026amp;\u0026amp; detections.multiHandLandmarks[0]) { const thumbToPinky = dist( detections.multiHandLandmarks[0][4].x * width, detections.multiHandLandmarks[0][4].y * height, detections.multiHandLandmarks[0][20].x * width, detections.multiHandLandmarks[0][20].y * height ); if (Math.abs(thumbToPinky) \u0026lt; 40) { const x = Math.abs(detections.multiHandLandmarks[0][20].x); const y = Math.abs(detections.multiHandLandmarks[0][20].y); const z = Math.abs(detections.multiHandLandmarks[0][20].z); depth.value((1 - x) / 0.3); } } } function update() { if (detections \u0026amp;\u0026amp; detections.multiHandLandmarks[1]) { const thumbToPinky = dist( detections.multiHandLandmarks[0][4].x * width, detections.multiHandLandmarks[0][4].y * height, detections.multiHandLandmarks[0][20].x * width, detections.multiHandLandmarks[0][20].y * height ); if (record \u0026amp;\u0026amp; Math.abs(thumbToPinky) \u0026lt; 40) { const x = width - detections.multiHandLandmarks[1][20].x * width; const y = detections.multiHandLandmarks[1][20].y * height; let dx = abs(x - (x - 5)); let dy = abs(y - (y - 5)); speed = constrain((dx + dy) / (2 * (width - height)), 0, 1); points.push({ worldPosition: treeLocation([x, y, depth.value()], { from: \u0026#34;SCREEN\u0026#34;, to: \u0026#34;WORLD\u0026#34;, }), speed: speed, pcolor: color( Math.random() * 255, Math.random() * 255, Math.random() * 255 ), }); } } } function boxBrush(point) { push(); noStroke(); fill(point.pcolor); box(1); pop(); } function keyPressed() { if (key === \u0026#34;r\u0026#34;) { record = !record; } if (key === \u0026#34;p\u0026#34;) { escorzo = !escorzo; escorzo ? perspective() : ortho(); } if (key == \u0026#34;c\u0026#34;) { points = []; } } Ml5.js hand let handpose; let video; let myResults = []; let angleX = 0; let angleY = 0; function setup() { createCanvas(640, 580, WEBGL); video = createCapture(VIDEO); video.size(width, height); video.hide(); handpose = ml5.handpose(video, modelReady); handpose.on(\u0026#34;predict\u0026#34;, (results) =\u0026gt; { myResults = results; }); push(); } function modelReady() { console.log(\u0026#34;Model ready!\u0026#34;); } function draw() { pop(); image(video, 0, 0, width, height); background(51); // rotateX(angleX); // rotateY(angleY); update(); textureMode(NORMAL); texture(earth); sphere(200); push(); } function update() { if (myResults \u0026amp;\u0026amp; myResults[0]) { const fingerDistance = Math.hypot(myResults[0].landmarks[4][0] + myResults[0].landmarks[4][1]) - Math.hypot(myResults[0].landmarks[20][0] + myResults[0].landmarks[20][1]); if (Math.abs(fingerDistance) \u0026lt; 40) { const x = Math.abs((width - myResults[0].landmarks[8][0]) / width) - 0.5; const y = Math.abs(myResults[0].landmarks[8][1] / height) - 0.3; rotateX(map(x / 60, 0, 0.5, 0, 2 * Math.PI)); rotateY(map(y / 60, 0, 0.5, 0, 2 * Math.PI)); } } } function preload() { earth = loadImage( \u0026#34;https://lh6.googleusercontent.com/MKWuIXLwcIXgwmrKrnjgCFEjna_8kFePKfWJlhOQLpBZ3pagPVPjxyHxZPHs2CTGMm1sdKLx_WGkjVhnDF_L9EQbata6o2Cw0dtIvNYz-yQG_YJXNfpWff_HbdsNtqkWAia6jwG7aLWDbJbn6w\u0026#34; ); } Demo video IV. Conclusiones # Para concluir, podemos resaltar en nuestras investigaciones que el sector de la interacción tridimensional entre usuario y máquina es rotundamente interesante desde la extracción de los datos hasta la manipulación y múltiples resultados posibles con estos. Con respecto al Finger Tracking, encontramos que es un campo que sigue en crecimiento y desarrollo a día hoy y su propuesta en términos de interacción vía gestos es ambiciosa, voraz y el camino a seguir. Por último, resaltar iniciativas como MediaPipe y M5l.js que buscan masificar el uso de Inteligencia Artificial y Machine Learning mediante un API sencilla y fácil de implementar en relación a la computación gráfica.\nEs importante que sigan surgiendo estas iniciativas de código abierto con el fin de educar y avanzar la tecnología para todos.\nV. Referencias # Wikipedia contributors. (2022, 22 abril). Texture mapping. Wikipedia. https://en.wikipedia.org/wiki/Texture_mapping. Wikipedia Wikipedia contributors. (2022, mayo 16). Spherical coordinate system. Wikipedia. https://en.wikipedia.org/wiki/Spherical_coordinate_system Wikipedia Hands. (2022). Mediapipe. https://google.github.io/mediapipe/solutions/hands. MediaPipe Hands Handpose. (2022). ml5.js. https://learn.ml5js.org/#/reference/handpose. Ml5.js handpose Wikipedia contributors. (2021, 29 agosto). Finger tracking. Wikipedia. https://en.wikipedia.org/wiki/Finger_tracking Wikipedia Wikipedia contributors. (2022, marzo 19). 3D pose estimation. Wikipedia. https://en.wikipedia.org/wiki/3D_pose_estimation Wikipedia "},{"id":5,"href":"/VisualComputing-Showcase/workshops/workshop3FR/","title":"Workshop3FR","section":"Workshops","content":" Shaders # Texturing and Coloring # I. Introducción # Las texturas y su respectivo mapeo en diferentes objetos dan la posibilidad de entender mejor la realidad de estos y modelarlos a nivel de proceso. En esta sección, encontraremos una explicación simple a este proceso y cuáles son los posibles factores, conceptos o parámetros que se deben tener en cuenta cuando se habla de textura, color, formas y mapeo.\nII. Contextualización # En primer lugar, el mapeo UV de una textura a un objeto o forma es el proceso de transformación que afrontan las coordenadas de la textura. Las coordenadas UV como las coordenadas de la imagen o gráfico de textura en dos dimensiones deben mapearse a las coordenadas y, z y x si es el caso de un objeto tridimensional. Para este proceso, los shaders construidos y presentados en este informe utilizan la librería de p5.treegl para la ubicación de las coordenadas Tree.texcoords2.\nEn cuanto al color y cómo se ve este representado a niveles de programación, este se obtiene en el shader desde la textura y utilizando las coordenadas del vertice o de la librería (como en este caso) se obtiene un vector con los valores de color RGB junto con el alpha. En el primer ejemplo construido, realizamos un gradiente utilizando canales de color con una intensidad acorde a las coordenadas y mapeando a un conjunto de elipses en dos dimensiones. En el segundo ejemplo construido, utilizamos una imagen como textura y obtenemos el vector de color o texel para la creación de filtros acordes a la luminosidad. El filtro Luma sirve como una referencia a la luminosidad y su implementación está inspirada en la sección de texturing de Visual Computing. El filtro HSV valor V es un ejemplo sencillo ya que toma el mayor valor en los canales de color, mientras que el filtro de interpolado utiliza tanto color como las coordenadas UV para determinar el resultado final del píxel.\nIII. Resultados # La implementación utilizando p5.js y el editor web realizada para los casos anteriores se muestra a continuación:\n.js let uvShader; let selection_box; function preload() { uvShader = readShader(\u0026#39;shaders/uv.frag\u0026#39;, { matrices: Tree.pmvMatrix, varyings: Tree.texcoords2 }); } function setup() { createCanvas(300, 300, WEBGL); noStroke(); textureMode(NORMAL); selection_box = createSelect(); selection_box.position(10, 10); selection_box.style(\u0026#39;color\u0026#39;, \u0026#39;black\u0026#39;); selection_box.option(\u0026#39;None\u0026#39;); selection_box.option(\u0026#39;RG\u0026#39;); selection_box.option(\u0026#39;GB\u0026#39;); selection_box.option(\u0026#39;RB\u0026#39;); selection_box.changed(changeSelection); } function draw() { background(1); shader(uvShader); quad(-width / 2, -height / 2, width / 2, -height / 2, width / 2, height / 2, -width / 2, height / 2); noStroke(); for (let i = 0; i \u0026lt; 10; i ++) { shader(uvShader); ellipse(0, 0, 20, 80, 1000); rotate(PI/5); } } function changeSelection() { let val = selection_box.value(); if (val == \u0026#39;RG\u0026#39;) { uvShader.setUniform(\u0026#39;GB\u0026#39;, false); uvShader.setUniform(\u0026#39;RB\u0026#39;, false); uvShader.setUniform(\u0026#39;RG\u0026#39;, true); } else if (val == \u0026#39;GB\u0026#39;) { uvShader.setUniform(\u0026#39;RG\u0026#39;, false); uvShader.setUniform(\u0026#39;RB\u0026#39;, false); uvShader.setUniform(\u0026#39;GB\u0026#39;, true); } else if (val == \u0026#39;RB\u0026#39;) { uvShader.setUniform(\u0026#39;RG\u0026#39;, false); uvShader.setUniform(\u0026#39;GB\u0026#39;, false); uvShader.setUniform(\u0026#39;RB\u0026#39;, true); } else { uvShader.setUniform(\u0026#39;RG\u0026#39;, false); uvShader.setUniform(\u0026#39;GB\u0026#39;, false); uvShader.setUniform(\u0026#39;RB\u0026#39;, false); } } .frag precision mediump float; varying vec2 texcoords2; uniform bool RG; uniform bool GB; uniform bool RB; void main() { if(RG) { gl_FragColor = vec4(texcoords2.yx, 0.0, 1.0); } else if (GB) { gl_FragColor = vec4(0.0, texcoords2.xy, 1.0); } else if (RB) { gl_FragColor = vec4(texcoords2.x, 0.0, texcoords2.y, 1.0); } } .js let coloringShader; let image; let selection_box; function preload() { coloringShader = readShader(\u0026#39;shaders/coloring.frag\u0026#39;, { varyings: Tree.texcoords2 }); // image source: https://en.wikipedia.org/wiki/HSL_and_HSV#/media/File:Fire_breathing_2_Luc_Viatour.jpg image = loadImage(\u0026#39;https://upload.wikimedia.org/wikipedia/commons/thumb/0/02/Fire_breathing_2_Luc_Viatour.jpg/1280px-Fire_breathing_2_Luc_Viatour.jpg\u0026#39;); } function setup() { createCanvas(700, 500, WEBGL); noStroke(); textureMode(IMAGE); shader(coloringShader); selection_box = createSelect(); selection_box.position(10, 10); selection_box.style(\u0026#39;color\u0026#39;, \u0026#39;black\u0026#39;); selection_box.option(\u0026#39;None\u0026#39;); selection_box.option(\u0026#39;Luma\u0026#39;); selection_box.option(\u0026#39;HSV V\u0026#39;); selection_box.option(\u0026#39;Interpolated\u0026#39;); selection_box.changed(changeSelection); coloringShader.setUniform(\u0026#39;texture\u0026#39;, image); } function draw() { background(0); quad(-width / 2, -height / 2, width / 2, -height / 2, width / 2, height / 2, -width / 2, height / 2); } function changeSelection() { let val = selection_box.value(); if (val == \u0026#39;Luma\u0026#39;) { coloringShader.setUniform(\u0026#39;hsv\u0026#39;, false); coloringShader.setUniform(\u0026#39;interpolated\u0026#39;, false); coloringShader.setUniform(\u0026#39;luma\u0026#39;, true); } else if (val == \u0026#39;HSV V\u0026#39;) { coloringShader.setUniform(\u0026#39;luma\u0026#39;, false); coloringShader.setUniform(\u0026#39;interpolated\u0026#39;, false); coloringShader.setUniform(\u0026#39;hsv\u0026#39;, true); } else if (val == \u0026#39;Interpolated\u0026#39;) { coloringShader.setUniform(\u0026#39;luma\u0026#39;, false); coloringShader.setUniform(\u0026#39;hsv\u0026#39;, false); coloringShader.setUniform(\u0026#39;interpolated\u0026#39;, true); } else { coloringShader.setUniform(\u0026#39;luma\u0026#39;, false); coloringShader.setUniform(\u0026#39;hsv\u0026#39;, false); coloringShader.setUniform(\u0026#39;interpolated\u0026#39;, false); } } .frag precision mediump float; uniform bool luma; uniform bool hsv; uniform bool interpolated; uniform sampler2D texture; varying vec2 texcoords2; float luma_function(vec3 texel) { return 0.299 * texel.r + 0.587 * texel.g + 0.114 * texel.b; } float hsv_function(vec3 texel) { return max(max(texel.r, texel.b), max(texel.r,texel.g)); } float interpolated_function(vec4 texel, vec2 texcoords2) { if (abs(texcoords2.x - texcoords2.y) \u0026lt; 0.2) { return texel.r; } else if (abs(texcoords2.x - texcoords2.y) \u0026lt; 0.5) { return texel.g; } else { return texel.b; } } void main() { vec4 texel = texture2D(texture, texcoords2); if (luma) { gl_FragColor = vec4((vec3(luma_function(texel.rgb))), 1.0); } else if (hsv) { gl_FragColor = vec4((vec3(hsv_function(texel.rgb))), 1.0); } else if (interpolated) { gl_FragColor = vec4((vec3(interpolated_function(texel, texcoords2))), 1.0); } else { gl_FragColor = texel; } } Image Processing # I. Introducción # El procesamiento de imágenes es una actividad en donde se aplican variedades de procesos a imágenes digitales para obtener o generar efectos e información. Esta actividad se realiza a nivel de píxeles y por tanto es computacionalmente costosa dependiendo de la resolución de la imagen.\nPor tanto, las tarjetas gráficas son el dispositivo ideal para realizar estos procesamientos debido a su capacidad de cómputo y los programas o softwares dedicados deben ser idealmente construidos para que se adapten a este dispositivo y generar así un mejor rendimiento como es el caso de los shaders.\nII. Contextualización # En primer lugar, se generó un efecto BLUR o de desefonque utilizando como guía la convolución presentada en la sección de Image Processing - Visual Computing que utilizaba los \u0026lsquo;vecinos\u0026rsquo; o píxeles alrededor del actual para calcular un resultado dada una máscara o kernel de cierto filtro. La máscara utilizada en BLUR es una específica para generar este efecto: [1 / 16, 1 / 8, 1 / 16, 1 / 8, 1 / 4, 1 / 8, 1 / 16, 1 / 8, 1 / 16]. Es importante notar que la obtención de los vecinos está considerando que para aquellos píxeles en el borde no se obtendrá un resultado distinto en el cálculo de color si el píxel vecino no es encontrado.\nTambién fue implementado un magnifier o un efecto de lupa, inspirado en un shader de ShaderToy que utiliza un efecto de círculo para el ampliamiento de una región de píxeles sin perder la completitud de la imagen utilizando un parámetro de profundidad y un radio de la región amplificada.\nPor último, utilizamos un cálculo de distancias en el shader mediante la función distance() para generar un círculo con el aplicado de un filtro y/o máscara de detección de bordes a una región de interés limitada en la imagen que es variable y dependiente de la posición actual del mouse en el programa.\nIII. Resultados # La implementación utilizando p5.js y el editor web realizada para los casos anteriores se muestra a continuación:\n.js let filterShader; let image; let video; let video_on; let selection_box; let magnifier; let region; function preload() { video = createVideo([\u0026#39;./images/earth.webm\u0026#39;]); video.hide(); filterShader = readShader(\u0026#39;./shaders/filter.frag\u0026#39;, { varyings: Tree.texcoords2 }); image = loadImage(\u0026#39;./images/vangogh.jpg\u0026#39;); } function setup() { createCanvas(650, 500, WEBGL); noStroke(); textureMode(NORMAL); video_on = createCheckbox(\u0026#39;Video\u0026#39;, false); video_on.style(\u0026#39;color\u0026#39;, \u0026#39;white\u0026#39;); video_on.changed(() =\u0026gt; { if (video_on.checked()) { filterShader.setUniform(\u0026#39;texture\u0026#39;, video); video.loop(); } else { filterShader.setUniform(\u0026#39;texture\u0026#39;, image); video.pause(); } }); video_on.position(10, 30); selection_box = createSelect(); selection_box.position(10, 10); selection_box.style(\u0026#39;color\u0026#39;, \u0026#39;black\u0026#39;); selection_box.option(\u0026#39;None\u0026#39;); selection_box.option(\u0026#39;BLUR\u0026#39;); selection_box.option(\u0026#39;Luma\u0026#39;); selection_box.option(\u0026#39;HSV V\u0026#39;); selection_box.changed(changeSelection); magnifier = createCheckbox(\u0026#39;Magnifier\u0026#39;, false); magnifier.position(10, 50); magnifier.style(\u0026#39;color\u0026#39;, \u0026#39;white\u0026#39;); region = createCheckbox(\u0026#39;Region\u0026#39;, false); region.position(10, 70); region.style(\u0026#39;color\u0026#39;, \u0026#39;white\u0026#39;); shader(filterShader); filterShader.setUniform(\u0026#39;texture\u0026#39;, image); filterShader.setUniform(\u0026#39;mask\u0026#39;, [0, 0, 0, 0, 1, 0, 0, 0, 0]); emitTexOffset(filterShader, image, \u0026#39;texOffset\u0026#39;); } function draw() { background(0); if (magnifier.checked()) { let mouse_x = map(mouseX, 0, width, 0.0, 1.0); let mouse_y = map(mouseY, 0, height, 0.0, 1.0); filterShader.setUniform(\u0026#39;u_Resolution\u0026#39;, [width, height]); filterShader.setUniform(\u0026#39;u_Mouse\u0026#39;, [mouse_x, mouse_y]); filterShader.setUniform(\u0026#39;region\u0026#39;, false); filterShader.setUniform(\u0026#39;magnifier\u0026#39;, true); } else if (region.checked()) { let mouse_x = map(mouseX, 0, width, 0.0, 1.0); let mouse_y = map(mouseY, 0, height, 0.0, 1.0); filterShader.setUniform(\u0026#39;mask_sobel\u0026#39;, [-1, -1, -1, -1, 8, -1, -1, -1, -1]); filterShader.setUniform(\u0026#39;u_Resolution\u0026#39;, [width, height]); filterShader.setUniform(\u0026#39;u_Mouse\u0026#39;, [mouse_x, mouse_y]); filterShader.setUniform(\u0026#39;magnifier\u0026#39;, false); filterShader.setUniform(\u0026#39;region\u0026#39;, true); } else { filterShader.setUniform(\u0026#39;region\u0026#39;, false); filterShader.setUniform(\u0026#39;magnifier\u0026#39;, false); } quad(-width / 2, -height / 2, width / 2, -height / 2, width / 2, height / 2, -width / 2, height / 2); } function changeSelection() { let val = selection_box.value(); if (val == \u0026#39;Luma\u0026#39;) { filterShader.setUniform(\u0026#39;hsv\u0026#39;, false); filterShader.setUniform(\u0026#39;mask\u0026#39;, [0, 0, 0, 0, 1, 0, 0, 0, 0]); filterShader.setUniform(\u0026#39;luma\u0026#39;, true); } else if (val == \u0026#39;HSV V\u0026#39;) { filterShader.setUniform(\u0026#39;luma\u0026#39;, false); filterShader.setUniform(\u0026#39;mask\u0026#39;, [0, 0, 0, 0, 1, 0, 0, 0, 0]); filterShader.setUniform(\u0026#39;hsv\u0026#39;, true); } else if (val == \u0026#39;BLUR\u0026#39;) { filterShader.setUniform(\u0026#39;luma\u0026#39;, false); filterShader.setUniform(\u0026#39;hsv\u0026#39;, false); filterShader.setUniform(\u0026#39;mask\u0026#39;, [1 / 16, 1 / 8, 1 / 16, 1 / 8, 1 / 4, 1 / 8, 1 / 16, 1 / 8, 1 / 16]); } else { filterShader.setUniform(\u0026#39;luma\u0026#39;, false); filterShader.setUniform(\u0026#39;hsv\u0026#39;, false); filterShader.setUniform(\u0026#39;mask\u0026#39;, [0, 0, 0, 0, 1, 0, 0, 0, 0]); } } .frag precision mediump float; const float radius = 2.; const float depth = radius / 2.; const float radius_region = 0.2; uniform sampler2D texture; uniform vec2 texOffset; uniform float mask[9]; uniform bool luma; uniform bool hsv; uniform bool magnifier; uniform vec2 u_Resolution; uniform vec2 u_Mouse; uniform bool region; uniform float mask_sobel[9]; varying vec2 texcoords2; float luma_function(vec3 texel) { return 0.299 * texel.r + 0.587 * texel.g + 0.114 * texel.b; } float hsv_function(vec3 texel) { return max(max(texel.r, texel.b), max(texel.r,texel.g)); } float sobel_function(vec3 texel) { return 0.0; } void main() { vec2 tc0 = texcoords2 + vec2(-texOffset.s, -texOffset.t); vec2 tc1 = texcoords2 + vec2( 0.0, -texOffset.t); vec2 tc2 = texcoords2 + vec2(+texOffset.s, -texOffset.t); vec2 tc3 = texcoords2 + vec2(-texOffset.s, 0.0); vec2 tc4 = texcoords2 + vec2( 0.0, 0.0); vec2 tc5 = texcoords2 + vec2(+texOffset.s, 0.0); vec2 tc6 = texcoords2 + vec2(-texOffset.s, +texOffset.t); vec2 tc7 = texcoords2 + vec2( 0.0, +texOffset.t); vec2 tc8 = texcoords2 + vec2(+texOffset.s, +texOffset.t); vec4 rgba[9]; rgba[0] = texture2D(texture, tc0); rgba[1] = texture2D(texture, tc1); rgba[2] = texture2D(texture, tc2); rgba[3] = texture2D(texture, tc3); rgba[4] = texture2D(texture, tc4); rgba[5] = texture2D(texture, tc5); rgba[6] = texture2D(texture, tc6); rgba[7] = texture2D(texture, tc7); rgba[8] = texture2D(texture, tc8); vec4 convolution; for (int i = 0; i \u0026lt; 9; i++) { convolution += rgba[i] * mask[i]; } if (luma) { gl_FragColor = vec4((vec3(luma_function(convolution.rgb))), 1.0); } else if (hsv) { gl_FragColor = vec4((vec3(hsv_function(convolution.rgb))), 1.0); } else { gl_FragColor = vec4(convolution.rgb, 1.0); } if (magnifier) { vec2 uv = gl_FragCoord.xy/(u_Resolution.xy * 2.0); vec2 center = vec2(u_Mouse.x, 1.0 - u_Mouse.y); float ax = ((uv.x - center.x) * (uv.x - center.x)) / (0.2 * 0.2) + ((uv.y - center.y) * (uv.y - center.y)) / (0.1 / (u_Resolution.x / u_Resolution.y)); float dx = 0.0 + (-depth / radius) * ax + (depth / (radius * radius)) * ax * ax; float f = (ax + dx); if (ax \u0026gt; radius) { f = ax; } vec2 magnifierArea = center + (uv - center) * f / ax; vec2 magnifier_r = vec2(magnifierArea.x, 1.0 - magnifierArea.y); vec4 color = texture2D(texture, magnifier_r); gl_FragColor = vec4(color.rgb, 1.0); } else if (region) { vec2 uv = gl_FragCoord.xy / (u_Resolution.xy * 2.0); vec2 center = vec2(u_Mouse.x, 1.0 - u_Mouse.y); if (distance(center, uv) \u0026lt; radius_region ) { vec4 convolution_sobel; for (int i = 0; i \u0026lt; 9; i++) { convolution_sobel += rgba[i] * mask_sobel[i]; } gl_FragColor = vec4(vec3(convolution_sobel.rgb), 1.0); } } } Procedural Texturing # I. Introducción # Las texturas son comúnmente identificadas como simples imágenes precargadas y sin embargo, es posible generar una textura a partir de un patrón algorítmico y posteriormente realizar el mapeo de este a un objeto o forma. Es necesario utilizar un espacio fuera de pantalla para renderizar y generar la textura y el espacio de pantalla donde se presetarán los resultados del mapeo.\nII. Contextualización # En esta sección fue creada una textura inspirada en un patrón hexagonal tomado de Shadertoy y mapeada a una esfera. Es importante resaltar que el patrón original utiliza el tiempo del programa en segundos como parámetro para la visualización de un fenómeno o ilusión óptica que involucra los colores blanco y negro junto con una percepción de relieve y completitud. Sin embargo, esta adaptación utiliza la posición del mouse en el programa para mapear a valores del 1 a 10 y utilizar una función sinusiodal que reproduce resultados entre 0 y 1 para generar los valores negros y blancos en repetidas ocasiones durante una única interacción completa del mouse en el programa.\nIII. Resultados # La implementación utilizando p5.js y el editor web realizada para los casos anteriores se muestra a continuación:\n.js let pg; let textureShader; function preload() { textureShader = readShader(\u0026#39;./shaders/texture.frag\u0026#39;, { matrices: Tree.NONE, varyings: Tree.NONE }); } function setup() { createCanvas(400, 400, WEBGL); pg = createGraphics(400, 400, WEBGL); textureMode(NORMAL); noStroke(); pg.noStroke(); pg.textureMode(NORMAL); pg.shader(textureShader); pg.emitResolution(textureShader); pg.quad(-1, -1, 1, -1, 1, 1, -1, 1); deltaTime = 0; texture(pg); } function draw() { background(0); rotateY(millis() / 10000); sphere(100); } function mouseMoved() { textureShader.setUniform(\u0026#39;u_Mouse\u0026#39;, float(map(mouseX, 0, width, 0, 10))); map(); pg.quad(-1, -1, 1, -1, 1, 1, -1, 1); } .frag precision mediump float; uniform vec2 u_resolution; uniform float u_Mouse; float facetHexPattern(vec2 u) { vec2 s = vec2(1., 1.732); vec2 a = s-2.*mod(u,s); vec2 b = s-2.*mod(u+s*vec2(0.5,0.5),s); return(0.7+0.2*sin(u_Mouse*1.1)-0.5*min(dot(a,a),dot(b,b))); } void main(void) { vec2 u = 22.*gl_FragCoord.xy/u_resolution.xy; gl_FragColor = vec4(vec3(facetHexPattern(u)), 1.0); } IV. Conclusiones # Para concluir, este informe puede resaltar la importancia y motivación en cuando a renderización en los shaders ya que estos ejercicios son reproducibles de una manera oportuna y eficiente debido al uso del tipo de hardware adecuado. También, es posible resaltar que librerías como p5.js y Treegl son realmente útiles y dan soluciones muy simples a problemáticas complejas como lo es computación visual, es por esto que la reproducción de códigos y creación de shaders es obsoleta si no se tienen unas bases claras en cuanto al medio/bajo nivel de conceptos como la rasterización.\nV. Referencias # Treegl | Visual Computing. (2022). GitHub - VisualComputing/p5.treegl. GitHub. https://github.com/VisualComputing/p5.treegl Visual Computing Texturing | Visual Computing. (2022, 7 junio). Visual Computing. https://visualcomputing.github.io/docs/shaders/texturing/ Visual Computing Wikipedia contributors. (2022, 26 abril). Gaussian blur. Wikipedia. https://en.wikipedia.org/wiki/Gaussian_blur Wikipedia Image Processing | Visual Computing. (2022, 6 junio). Visual Computing. https://visualcomputing.github.io/docs/shaders/image_processing/ Visual Computing Shadertoy. (2022). Magnifier. https://www.shadertoy.com/view/llsSz7 Shadertoy Shadertoy. (2022). Hexagonal. https://www.shadertoy.com/view/Wstfzj Shadertoy "},{"id":6,"href":"/VisualComputing-Showcase/workshops/workshop3NA/","title":"Workshop3FR","section":"Workshops","content":" Shaders # I. Introducción # Segun Patricio Gonzalez \u0026amp; Jen Lowe, los shaders son conjuntos de instrucciones que se ejecutan todas a la vez para cada píxel de la pantalla. Según esto, el código escrito debe comportarse de forma diferente dependiendo de la posición del píxel en la pantalla, operando como una función que recibe una posición y devuelve un color, y cuando se compila se ejecuta extraordinariamente rápido, esto debido a la capacidad de cómputo de las GPU, que realizan cálculos sobre hardware paralelo y funciones matemáticas especiales aceleradas por el mismo, en lugar de software.\nHay diferentes tipos de shaders, pero hay dos que se utilizan habitualmente para crear gráficos en la web: Vertex Shaders y Fragment Shaders. Los Vertex Shaders transforman las posiciones de las formas en coordenadas de dibujo 3D. Los Fragment Shaders calculan las representaciones de los colores de una forma y otros atributos.\nGLSL es el estándar específico de los shaders; el lenguaje Shader tiene una única función principal que devuelve un color al final, este color del píxel se asigna a la variable global reservada gl_FragColor, la cual contiene cuatro argumentos correspondientes a los canales rojo, verde, azul y alfa, valores que están normalizados. Este lenguaje incorpora además de variables (como gl_FragColor), funciones y tipos, como vec4 que representa un vector de cuatro dimensiones (similarmente existen vec3 y vec2, o tipos reconocidos como float o bool).\nLa GPU gestiona un gran número de hilos paralelos, cada uno responsable de asignar el color a una fracción de la imagen total, y aunque cada hilo paralelo es ciego a los demás, se necesita poder enviar algunas entradas de la CPU a todos los hilos. Estas entradas van a ser uniformes a todos los hilos siguiendo la arquitectura de la tarjeta gráfica y se establecen como de sólo lectura. Estas entradas se llaman uniformes y se verán comunmente en cada implementación de fragment shader, por ejemplo u_time que lleva el tiempo en milisegundos desde el inicio del programa o uResolution que especifica el tamaño del canvas.\nSimilar a como GLSL otorga una salida por defecto, gl_FragColor, también otorga una entrada por defecto, gl_FragCoord, que contiene las coordenadas de pantalla del píxel o fragmento de pantalla sobre el que está trabajando el hilo activo. Con gl_FragCoord, se sabe dónde está trabajando un hilo dentro del billboard. Debido a que las coordenadas son diferentes de un hilo a otro, en su lugar gl_FragCoord se denomina variante.\nPara leer shaders desde p5.js se hace uso de la librería p5.treegl, mediante la función readShader, que carga un fragment shader desde la ruta del archivo y devuelve un shader de p5. Esta recibe además la precisión de los flotantes del vertex shader, las matrices uniformes del vertex shader y los atributos del vertex shader para ser interpoladas al fragment shader. El vertex shader detrás del p5.shader se genera automáticamente a partir de una llamada a otra función denominada parseVertexShader.\nTexturing and Coloring # I. Introducción # Una textura es una imagen mapeada sobre la superficie de una forma. El mapeo de imagenes en diferentes objetos en espacios bidimensionales y tridimensionales se puede realizar de diferentes formas en GLSL, esto se explorará a continuación.\nII. Contextualización # Para el caso de visualización UV, la textura se define en el llamado espacio uv. Las letras \u0026ldquo;U\u0026rdquo; y \u0026ldquo;V\u0026rdquo; denotan los ejes de la textura 2D resultado del mapeo realizado, porque \u0026ldquo;X\u0026rdquo;, \u0026ldquo;Y\u0026rdquo; y \u0026ldquo;Z\u0026rdquo; ya se utilizan para denotar los ejes del objeto 3D en model space. Así, las coordenadas de cada vértice texcoords2, de dos formas diferentes son mapeadas en cada fragment shader para representar un color determinado, creando un tipo de gradiente del color.\nPor otro lado, el muestreo de una textura específica en coordenadas de vértice dadas requiere la llamada a glsl texture2D dentro del shader. La función texture2D devuelve un texel, es decir, el valor de color de la textura para las coordenadas dadas. La función tiene un parámetro de entrada del tipo sampler2D y un parámetro de entrada del tipo vec2, correspondientes al uniforme al que está ligada la textura y las coordenadas bidimensionales del texel a buscar respectivamente. Esta es utilizada entonces para calcular una visualización de la luminosidad de una imagen dada, por ejemplo.\nAsí, para la segunda implementación realizada, se muestrea una textura específica, de manera que el texel obtenido se opera de diferentes formas para aplicar un filtro a la imagen inicial. Se implementaron entonces cinco casos diferentes: inversión de los colores de cada pixel, obtención del valor HSV V de un pixel, obtención del valor HSL L o luminosidad del HSL, obtención del luma de un pixel y reducción de los canales rojo, verde y/o azul de un pixel mediante sliders respectivamente.\nIII. Resultados # La implementación utilizando p5.js, y el editor web, realizada para los casos anteriores se muestra a continuación:\nsketch.js let easycam; let uvShader; function preload() { // Define geometry in world space (i.e., matrices: Tree.pmvMatrix). // The projection and modelview matrices may be emitted separately // (i.e., matrices: Tree.pMatrix | Tree.mvMatrix), which actually // leads to the same gl_Position result. // Interpolate only texture coordinates (i.e., varyings: Tree.texcoords2). // see: https://github.com/VisualComputing/p5.treegl#handling uvShader = readShader(\u0026#34;uv.frag\u0026#34;, { matrices: Tree.pmvMatrix, varyings: Tree.texcoords2, }); uvShader2 = readShader(\u0026#34;uv2.frag\u0026#34;, { matrices: Tree.pmvMatrix, varyings: Tree.texcoords2, }); } function setup() { createCanvas(500, 500, WEBGL); textureMode(NORMAL); } function draw() { background(0); orbitControl(); shader(uvShader2); noStroke(); quad( -width / 2, -height / 2, width / 2, -height / 2, width / 2, height / 2, -width / 2, height / 2 ); shader(uvShader); triangle(-width / 2, height / 2, width / 2, height / 2, 0, -height / 2); } function mouseWheel(event) { //comment to enable page scrolling return false; } uv.frag precision mediump float; // the texture coordinates varying was defined in // the vertex shader by treegl readShader() // open your console and \u0026amp; see! varying vec2 texcoords2; void main() { gl_FragColor = vec4(texcoords2.x, 0, texcoords2.y, 1.0); // if (texcoords2.y \u0026gt; 0.25 \u0026amp;\u0026amp; texcoords2.y \u0026lt; 0.50){ // gl_FragColor = vec4(texcoords2.x, 0, texcoords2.y, 1.0); // }else if (texcoords2.y \u0026gt; 0.50 \u0026amp;\u0026amp; texcoords2.x \u0026gt; 0.50 \u0026amp;\u0026amp; texcoords2.x \u0026lt; 0.75){ // gl_FragColor = vec4(texcoords2.x, 1, texcoords2.y, 1.0); // }else if (texcoords2.y \u0026gt; 0.75 \u0026amp;\u0026amp; texcoords2.x \u0026lt; 0.25 ){ // gl_FragColor = vec4(texcoords2.y, 1, texcoords2.x, 1.0); // }else if (texcoords2.y \u0026lt; 0.75 \u0026amp;\u0026amp; texcoords2.x \u0026gt; 0.25 \u0026amp;\u0026amp; texcoords2.x \u0026lt; 0.50){ // gl_FragColor = vec4(texcoords2.x, 0, 1, 1.0); // }else if (texcoords2.y \u0026lt; 0.25 \u0026amp;\u0026amp; texcoords2.x \u0026lt; 0.50){ // gl_FragColor = vec4(texcoords2.x, 1, 0, 1.0); // }else if (texcoords2.x \u0026gt; 0.75){ // gl_FragColor = vec4(texcoords2.x, 0, texcoords2.y, 1.0); // }else{ // gl_FragColor = vec4(texcoords2.xy, 0, 1.0); // } } uv2.frag precision mediump float; // the texture coordinates varying was defined in // the vertex shader by treegl readShader() // open your console and \u0026amp; see! varying vec2 texcoords2; void main() { gl_FragColor = vec4(0.5, texcoords2.x, texcoords2.y, 1.0); // if (texcoords2.y \u0026gt; 0.25 \u0026amp;\u0026amp; texcoords2.y \u0026lt; 0.50){ // gl_FragColor = vec4(texcoords2.x, 0, texcoords2.y, 1.0); // }else if (texcoords2.y \u0026gt; 0.50 \u0026amp;\u0026amp; texcoords2.x \u0026gt; 0.50 \u0026amp;\u0026amp; texcoords2.x \u0026lt; 0.75){ // gl_FragColor = vec4(texcoords2.x, 1, texcoords2.y, 1.0); // }else if (texcoords2.y \u0026gt; 0.75 \u0026amp;\u0026amp; texcoords2.x \u0026lt; 0.25 ){ // gl_FragColor = vec4(texcoords2.y, 1, texcoords2.x, 1.0); // }else if (texcoords2.y \u0026lt; 0.75 \u0026amp;\u0026amp; texcoords2.x \u0026gt; 0.25 \u0026amp;\u0026amp; texcoords2.x \u0026lt; 0.50){ // gl_FragColor = vec4(texcoords2.x, 0, 1, 1.0); // }else if (texcoords2.y \u0026lt; 0.25 \u0026amp;\u0026amp; texcoords2.x \u0026lt; 0.50){ // gl_FragColor = vec4(texcoords2.x, 1, 0, 1.0); // }else if (texcoords2.x \u0026gt; 0.75){ // gl_FragColor = vec4(texcoords2.x, 0, texcoords2.y, 1.0); // }else{ // gl_FragColor = vec4(texcoords2.xy, 0, 1.0); // } } sketch.js let lumaShader; let img; let grey_scale; let rslider; let gslider; let easycam; let bslider; function preload() { lumaShader = readShader(\u0026#34;luma.frag\u0026#34;, { varyings: Tree.texcoords2 }); // image source: https://images.unsplash.com/photo-1610683468005-79e792efddd9?ixlib=rb-1.2.1\u0026amp;ixid=MnwxMjA3fDB8MHxzZWFyY2h8MTB8fGdsYXNzJTIwcGFuZWx8ZW58MHx8MHx8\u0026amp;w=1000\u0026amp;q=80 img = loadImage(\u0026#34;images/colors.jpg\u0026#34;); } function setup() { createCanvas(600, 800, WEBGL); rslider = createSlider(0, 1, 0, 0.01); rslider.position(10, 10); rslider.style(\u0026#34;width\u0026#34;, \u0026#34;80px\u0026#34;); gslider = createSlider(0, 1, 0, 0.01); gslider.position(10, 40); gslider.style(\u0026#34;width\u0026#34;, \u0026#34;80px\u0026#34;); bslider = createSlider(0, 1, 0, 0.01); bslider.position(10, 70); bslider.style(\u0026#34;width\u0026#34;, \u0026#34;80px\u0026#34;); noStroke(); textureMode(IMAGE); shader(lumaShader); inverted = createCheckbox(\u0026#34;inverse\u0026#34;, false); inverted.position(width - 100, 10); inverted.style(\u0026#34;color\u0026#34;, \u0026#34;white\u0026#34;); inverted.input(() =\u0026gt; lumaShader.setUniform(\u0026#34;inverted\u0026#34;, inverted.checked())); hsv = createCheckbox(\u0026#34;hsv\u0026#34;, false); hsv.position(width - 100, 30); hsv.style(\u0026#34;color\u0026#34;, \u0026#34;white\u0026#34;); hsv.input(() =\u0026gt; lumaShader.setUniform(\u0026#34;hsv\u0026#34;, hsv.checked())); hsl = createCheckbox(\u0026#34;hsl\u0026#34;, false); hsl.position(width - 100, 50); hsl.style(\u0026#34;color\u0026#34;, \u0026#34;white\u0026#34;); hsl.input(() =\u0026gt; lumaShader.setUniform(\u0026#34;hsl\u0026#34;, hsl.checked())); luma = createCheckbox(\u0026#34;luma\u0026#34;, false); luma.position(width - 100, 70); luma.style(\u0026#34;color\u0026#34;, \u0026#34;white\u0026#34;); luma.input(() =\u0026gt; lumaShader.setUniform(\u0026#34;luma\u0026#34;, luma.checked())); rslider.changed(() =\u0026gt; { lumaShader.setUniform(\u0026#34;r_scale\u0026#34;, rslider.value()); }); gslider.changed(() =\u0026gt; { lumaShader.setUniform(\u0026#34;g_scale\u0026#34;, gslider.value()); }); bslider.changed(() =\u0026gt; { lumaShader.setUniform(\u0026#34;b_scale\u0026#34;, bslider.value()); }); lumaShader.setUniform(\u0026#34;texture\u0026#34;, img); } function draw() { background(0); quad( -width / 2, -height / 2, width / 2, -height / 2, width / 2, height / 2, -width / 2, height / 2 ); } luma.frag precision mediump float; // uniforms are defined and sent by the sketch uniform bool inverted; uniform bool hsv; uniform bool luma; uniform bool hsl; uniform float r_scale; uniform float g_scale; uniform float b_scale; uniform sampler2D texture; // interpolated texcoord (same name and type as in vertex shader) varying vec2 texcoords2; // returns luma of given texel float lum(vec3 texel) { return 0.299 * texel.r + 0.587 * texel.g + 0.114 * texel.b; } float hsvV(vec3 texel){ return max(max(texel.r, texel.g), texel.b); } float hslL(vec3 texel){ float max = hsvV(texel); float min = min(min(texel.r, texel.g), texel.b); return (max + min)/2.0; } void main() { // texture2D(texture, texcoords2) samples texture at texcoords2 // and returns the normalized texel color vec4 texel = texture2D(texture, texcoords2); gl_FragColor = inverted ? vec4((vec3(1.0, 1.0, 1.0) - texel.rgb), 1.0) : hsv ? vec4(vec3(hsvV(texel.rgb)), 1.0) : hsl ? vec4(vec3(hslL(texel.rgb)), 1.0) : luma ? vec4(vec3(lum(texel.rgb)), 1.0) : vec4(vec3(texel.r - r_scale, texel.g - g_scale, texel.b - b_scale), 1.0); } Image Processing # I. Introducción # El procesamiento digital de imágenes es el uso de un ordenador digital para procesar imágenes digitales mediante un algoritmo. Algunas transformaciones de imágenes digitales son\nFiltrado: Se utilizan para difuminar, dar nitidez a las imágenes digitales u otros efectos. Este filtrado se puede realizar mediante matrices de convolución específicamente diseñados en el dominio espacial o enmascaramiento de regiones de frecuencia específicas en el dominio de la frecuencia. Transformaciones afines: Permiten transformaciones básicas de la imagen, incluyendo escalamiento, rotación, traslación, reflejo y cizallamiento, como se ha visto en las lecciones anteriores. Desnaturalización de imágenes con morfología: Permiten la eliminación de ruido de las imágenes. Para este caso se realizó filtering mediante mascaras, de manera que se tienen en cuenta las coordenadas de textura de los pixeles vecinos.\nII. Contextualización # Como se ha mencionado en sesiones de clase, en WEBGL la textura se utiliza para implementar el procesamiento de imágenes y como se sabe hacer referencia a otros píxeles, se utilizaron matrices de convolución 3x3, en la que cada entrada de la matriz representa cuánto hay que multiplicar los 8 píxeles alrededor del píxel que estamos renderizando. Según esto, se aplicaron las matrices [-1, -1, -1, -1, 8, -1, -1, -1, -1], [1, 2, 1, 0, 0, 0, -1, -2, -1], [-2, -1, 0, -1, 1, 1, 0, 1, 2] para obtener filtros de blanco y negro, sobel horizontal y realzado respectivamente, las cuales fueron tomadas de WebGLFundamentals. Se utilizó la función distance de glsl para evidenciar el efecto de las máscaras en una región circular de la imagen que depende de la posición del mouse. Similarmente, se aplicaron otros efectos a la región como luma y el valor HSV V.\nAdemás, se implementó un efecto de lupa que amplía una región de pixeles con un radio y profundidad definidos. La implementación realizada fue basada en un shader realizado por el usuario wawan60 en la plataforma Shadertoy, la cual se encuentra en el siguiente enlace: https://www.shadertoy.com/view/llsSz7\nIII. Resultados # La implementación utilizando p5.js, y el editor web, realizada para los casos anteriores se muestra a continuación:\nsketch.js let maskShader; let img; let luma; let video_src; let video_on; let mask; function preload() { maskShader = readShader(\u0026#34;mask.frag\u0026#34;, { varyings: Tree.texcoords2 }); //source: https://images.unsplash.com/photo-1610683468005-79e792efddd9?ixlib=rb-1.2.1\u0026amp;ixid=MnwxMjA3fDB8MHxzZWFyY2h8MTB8fGdsYXNzJTIwcGFuZWx8ZW58MHx8MHx8\u0026amp;w=1000\u0026amp;q=80 img = loadImage(\u0026#34;images/colors.jpg\u0026#34;); } function setup() { // shaders require WEBGL mode to work createCanvas(600, 800, WEBGL); noStroke(); textureMode(NORMAL); other = createCheckbox(\u0026#34;region\u0026#34;, false); other.position(10, 10); other.style(\u0026#34;color\u0026#34;, \u0026#34;white\u0026#34;); other.input(() =\u0026gt; maskShader.setUniform(\u0026#34;region\u0026#34;, other.checked())); luma = createCheckbox(\u0026#34;luma\u0026#34;, false); luma.position(10, 30); luma.style(\u0026#34;color\u0026#34;, \u0026#34;white\u0026#34;); luma.input(() =\u0026gt; maskShader.setUniform(\u0026#34;luma\u0026#34;, luma.checked())); hsv = createCheckbox(\u0026#34;hsv\u0026#34;, false); hsv.position(10, 50); hsv.style(\u0026#34;color\u0026#34;, \u0026#34;white\u0026#34;); hsv.input(() =\u0026gt; maskShader.setUniform(\u0026#34;hsv\u0026#34;, hsv.checked())); sobel = createCheckbox(\u0026#34;sobel\u0026#34;, false); sobel.position(10, 70); sobel.style(\u0026#34;color\u0026#34;, \u0026#34;white\u0026#34;); emboss = createCheckbox(\u0026#34;emboss\u0026#34;, false); emboss.position(10, 90); emboss.style(\u0026#34;color\u0026#34;, \u0026#34;white\u0026#34;); shader(maskShader); maskShader.setUniform(\u0026#34;texture\u0026#34;, img); emitTexOffset(maskShader, img, \u0026#34;texOffset\u0026#34;); } function draw() { background(0); maskShader.setUniform(\u0026#34;mask\u0026#34;, [0, 0, 0, 0, 1, 0, 0, 0, 0]); const mx = map(mouseX, 0, width, 0.0, 1.0); const my = map(mouseY, 0, height, 0.0, 1.0); maskShader.setUniform(\u0026#34;uResolution\u0026#34;, [width, height]); maskShader.setUniform(\u0026#34;uMouse\u0026#34;, [mx, my]); //kernels taken from https://webglfundamentals.org/webgl/lessons/webgl-image-processing.html if (other.checked()) { maskShader.setUniform(\u0026#34;mask\u0026#34;, [-1, -1, -1, -1, 8, -1, -1, -1, -1]); } else if (sobel.checked()) { maskShader.setUniform(\u0026#34;mask\u0026#34;, [1, 2, 1, 0, 0, 0, -1, -2, -1]); } else if (emboss.checked()) { maskShader.setUniform(\u0026#34;mask\u0026#34;, [-2, -1, 0, -1, 1, 1, 0, 1, 2]); } else { maskShader.setUniform(\u0026#34;mask\u0026#34;, [0, 0, 0, 0, 1, 0, 0, 0, 0]); } quad( -width / 2, -height / 2, width / 2, -height / 2, width / 2, height / 2, -width / 2, height / 2 ); } mask.frag #ifdef GL_ES precision mediump float; #endif precision mediump float; uniform sampler2D texture; uniform vec2 texOffset; uniform vec2 uMouse; uniform bool region; uniform bool luma; uniform bool hsv; uniform vec2 uResolution; // holds the 3x3 kernel uniform float mask[9]; // we need our interpolated tex coord varying vec2 texcoords2; const float radius=0.2; const float depth=radius/2.; float lum(vec3 texel) { return 0.299 * texel.r + 0.587 * texel.g + 0.114 * texel.b; } float hsvV(vec3 texel){ return max(max(texel.r, texel.g), texel.b); } void main() { // 1. Use offset to move along texture space. // In this case to find the texcoords of the texel neighbours. vec2 tc0 = texcoords2 + vec2(-texOffset.s, -texOffset.t); vec2 tc1 = texcoords2 + vec2( 0.0, -texOffset.t); vec2 tc2 = texcoords2 + vec2(+texOffset.s, -texOffset.t); vec2 tc3 = texcoords2 + vec2(-texOffset.s, 0.0); // origin (current fragment texcoords) vec2 tc4 = texcoords2 + vec2( 0.0, 0.0); vec2 tc5 = texcoords2 + vec2(+texOffset.s, 0.0); vec2 tc6 = texcoords2 + vec2(-texOffset.s, +texOffset.t); vec2 tc7 = texcoords2 + vec2( 0.0, +texOffset.t); vec2 tc8 = texcoords2 + vec2(+texOffset.s, +texOffset.t); // 2. Sample texel neighbours within the rgba array vec4 rgba[9]; rgba[0] = texture2D(texture, tc0); rgba[1] = texture2D(texture, tc1); rgba[2] = texture2D(texture, tc2); rgba[3] = texture2D(texture, tc3); rgba[4] = texture2D(texture, tc4); rgba[5] = texture2D(texture, tc5); rgba[6] = texture2D(texture, tc6); rgba[7] = texture2D(texture, tc7); rgba[8] = texture2D(texture, tc8); // 3. Apply convolution kernel vec4 color = texture2D(texture, texcoords2); vec2 uv = gl_FragCoord.xy/(uResolution.xy); vec2 center = vec2(uMouse.x, 1.0 - uMouse.y); if(distance(uv, center) \u0026lt; radius){ vec4 convolution; for (int i = 0; i \u0026lt; 9; i++) { convolution += rgba[i]*mask[i]; } if (region){ gl_FragColor = vec4(vec3(convolution.r+convolution.g+convolution.b)/3.0, 1.0); }else if (luma){ gl_FragColor = vec4(vec3(lum(color.rgb)), 1.0); }else if(hsv){ gl_FragColor = vec4(vec3(hsvV(color.rgb)), 1.0); }else{ gl_FragColor = vec4(convolution.rgb, 1.0); } }else{ gl_FragColor = vec4(color.rgb, 1.0); } } sketch.js let maskShader; let img; let luma; let video_src; let video_on; let mask; function preload() { maskShader = readShader(\u0026#34;mask.frag\u0026#34;, { varyings: Tree.texcoords2, }); //source: https://kushfineart.com/artworks/categories/8/9501-metamorphosis-ii/ img = loadImage(\u0026#34;images/metamorphosis.jpg\u0026#34;); } function setup() { // shaders require WEBGL mode to work createCanvas(650, 550, WEBGL); noStroke(); textureMode(NORMAL); mask = createCheckbox(\u0026#34;magnifier\u0026#34;, false); mask.position(10, 10); mask.style(\u0026#34;color\u0026#34;, \u0026#34;white\u0026#34;); shader(maskShader); maskShader.setUniform(\u0026#34;texture\u0026#34;, img); emitTexOffset(maskShader, img, \u0026#34;texOffset\u0026#34;); } function draw() { background(0); if (mask.checked()) { maskShader.setUniform(\u0026#34;magnifier\u0026#34;, mask.checked()); const mx = map(mouseX, 0, width, 0.0, 1.0); const my = map(mouseY, 0, height, 0.0, 1.0); maskShader.setUniform(\u0026#34;uResolution\u0026#34;, [width, height]); maskShader.setUniform(\u0026#34;uMouse\u0026#34;, [mx, my]); } else { maskShader.setUniform(\u0026#34;magnifier\u0026#34;, mask.checked()); } quad( -width / 2, -height / 2, width / 2, -height / 2, width / 2, height / 2, -width / 2, height / 2 ); } mask.frag //adapted from https://www.shadertoy.com/view/llsSz7 by wawan60 precision mediump float; uniform sampler2D texture; uniform vec2 texOffset; uniform vec2 uMouse; uniform bool magnifier; uniform vec2 uResolution; // we need our interpolated tex coord varying vec2 texcoords2; const float radius=.5; const float depth=radius/1.8; void main() { vec4 texel = texture2D(texture, texcoords2); vec2 uv = gl_FragCoord.xy/(uResolution.xy); vec2 center = vec2(uMouse.x, 1.0 - uMouse.y); float ax = ((uv.x - center.x)* (uv.x - center.x)) / (0.2 * 0.2) + ((uv.y - center.y) * (uv.y - center.y)) / (0.1/ ( uResolution.x / uResolution.y )) ; float dx = (-depth/radius)*ax + (depth/(radius*radius))*ax*ax; float f = (ax + dx); if (ax \u0026gt; radius) f = ax; vec2 magnifierArea = center + (uv-center)*f/ax; vec2 magnifier_r = vec2(magnifierArea.x, 1.0 - magnifierArea.y); vec4 color = texture2D(texture, magnifier_r); gl_FragColor = magnifier ? vec4(color.rgb, 1.0) : vec4(texel.rgb, 1.0); } Procedural Texturing # I. Introducción # El texturizado procedimental busca generar una textura utilizando una descripción matemática o un algoritmo en lugar de datos almacenados directamente como una imagen, de manera que el resultado puede ser mapeado en una forma como una textura cualquiera. Este tipo de texturas se suelen utilizar para modelar representaciones superficiales o volumétricas de elementos naturales como la madera, el mármol, el granito, el metal, la piedra y otros. El texturizado procedimental requiere el uso de un objeto frame buffer que en p5.js se implementa como un objeto p5.Graphics.\nII. Contextualización # Para este caso, se utilizan dos texturas similares tomadas de Shadertoy, implementadas por Inigo Quilez (co-creador de Shadertoy), las cuales son mapeadas a una caja o cubo de p5. Ambas texturas utilizan el tiempo del programa (función millis() de p5.js) para cambiar la variable uniforme en el shader que permite simular el movimiento de la textura.\nIII. Resultados # La implementación utilizando p5.js y el editor web realizada para los casos anteriores se muestra a continuación:\nsketch.js let pg; let truchetShader; let x = 0; function preload() { // shader adapted from here: https://thebookofshaders.com/09/ truchetShader = readShader(\u0026#34;truchet.frag\u0026#34;, { matrices: Tree.NONE, varyings: Tree.NONE, }); } function setup() { createCanvas(700, 700, WEBGL); // create frame buffer object to render the procedural texture pg = createGraphics(700, 700, WEBGL); // textureMode(NORMAL); // noStroke(); noStroke(); textureMode(NORMAL); pg.noStroke(); pg.shader(truchetShader); pg.emitResolution(truchetShader); truchetShader.setUniform(\u0026#34;u_zoom\u0026#34;, 3); } function draw() { background(33); pg.quad(-1, -1, 1, -1, 1, 1, -1, 1); texture(pg); orbitControl(); // rotateX(millis() / 1000); // rotateY(millis() / 1000); emitMousePosition(truchetShader, \u0026#34;mouse\u0026#34;); truchetShader.setUniform(\u0026#34;u_time\u0026#34;, float(millis() / 50.0)); box(300, 300); } function mouseMoved() { //manual time update on mouse moved pg.quad(-1, -1, 1, -1, 1, 1, -1, 1); } truchet.frag // Author @iq - (https://iquilezles.org) - 2013 //taken from https://www.shadertoy.com/view/lsl3RH #ifdef GL_ES precision mediump float; #endif #define PI 3.14159265358979323846 uniform vec2 u_resolution; uniform float u_time; // uniform float u_zoom; const mat2 m = mat2( 0.80, 0.60, -0.60, 0.80 ); float noise( in vec2 p ) { return sin(p.x)*sin(p.y); } float fbm4( vec2 p ) { float f = 0.0; f += 0.5000*noise( p ); p = m*p*2.02; f += 0.2500*noise( p ); p = m*p*2.03; f += 0.1250*noise( p ); p = m*p*2.01; f += 0.0625*noise( p ); return f/0.9375; } float fbm6( vec2 p ) { float f = 0.0; f += 0.500000*(0.5+0.5*noise( p )); p = m*p*2.02; f += 0.250000*(0.5+0.5*noise( p )); p = m*p*2.03; f += 0.125000*(0.5+0.5*noise( p )); p = m*p*2.01; f += 0.062500*(0.5+0.5*noise( p )); p = m*p*2.04; f += 0.031250*(0.5+0.5*noise( p )); p = m*p*2.01; f += 0.015625*(0.5+0.5*noise( p )); return f/0.96875; } vec2 fbm4_2( vec2 p ) { return vec2(fbm4(p), fbm4(p+vec2(7.8))); } vec2 fbm6_2( vec2 p ) { return vec2(fbm6(p+vec2(16.8)), fbm6(p+vec2(11.5))); } //==================================================================== float fck( vec2 q, out vec4 ron ) { q += 0.03*sin( vec2(0.27,0.23)*u_time + length(q)*vec2(4.1,4.3)); vec2 o = fbm4_2( 0.9*q ); o += 0.04*sin( vec2(0.12,0.14)*u_time + length(o)); vec2 n = fbm6_2( 3.0*o ); ron = vec4( o, n ); float f = 0.5 + 0.5*fbm4( 1.8*q + 6.0*n ); return mix( f, f*f*f*3.5, f*abs(n.x) ); } void main(void ) { vec2 p = (2.0*gl_FragCoord.xy-u_resolution.xy)/u_resolution.y; float e = 2.0/u_resolution.y; vec4 on = vec4(0.0); float f = fck(p, on); vec3 col = vec3(0.0); col = mix( vec3(0.2,0.1,0.4), vec3(0.3,0.05,0.05), f ); col = mix( col, vec3(0.9,0.9,0.9), dot(on.zw,on.zw) ); col = mix( col, vec3(0.4,0.3,0.3), 0.2 + 0.5*on.y*on.y ); col = mix( col, vec3(0.0,0.2,0.4), 0.5*smoothstep(1.2,1.3,abs(on.z)+abs(on.w)) ); col = clamp( col*f*2.0, 0.0, 1.0 ); #if 0 // gpu derivatives - bad quality, but fast vec3 nor = normalize( vec3( dFdx(f)*u_resolution.x, 6.0, dFdy(f)*u_resolution.y ) ); #else // manual derivatives - better quality, but slower vec4 kk; vec3 nor = normalize( vec3( fck(p+vec2(e,0.0),kk)-f, 2.0*e, fck(p+vec2(0.0,e),kk)-f ) ); #endif vec3 lig = normalize( vec3( 0.9, 0.2, -0.4 ) ); float dif = clamp( 0.3+0.7*dot( nor, lig ), 0.0, 1.0 ); vec3 lin = vec3(0.90,0.90,0.40)*(nor.y*0.5+0.5) + vec3(0.15,0.10,0.05)*dif; col *= 1.2*lin; col = 1.0 - col; col = 1.1*col*col*col; gl_FragColor = vec4( col, 1.0 ); } truchet.frag // Author @iq - (https://iquilezles.org) - 2013 //Taken from https://www.shadertoy.com/view/4s23zz #ifdef GL_ES precision mediump float; #endif #define PI 3.14159265358979323846 uniform vec2 u_resolution; uniform float u_time; #define AA 1 float hash(vec2 p) // replace this by something better { p = fract( p*0.6180339887 ); p *= 25.0; return fract( p.x*p.y*(p.x+p.y) ); } // consider replacing this by a proper noise function float noise( in vec2 x ) { vec2 p = floor(x); vec2 f = fract(x); f = f*f*(3.0-2.0*f); float a = hash(p+vec2(0,0)); float b = hash(p+vec2(1,0)); float c = hash(p+vec2(0,1)); float d = hash(p+vec2(1,1)); return mix(mix( a, b,f.x), mix( c, d,f.x),f.y); } const mat2 mtx = mat2( 0.80, 0.60, -0.60, 0.80 ); float fbm4( vec2 p ) { float f = 0.0; f += 0.5000*(-1.0+2.0*noise( p )); p = mtx*p*2.02; f += 0.2500*(-1.0+2.0*noise( p )); p = mtx*p*2.03; f += 0.1250*(-1.0+2.0*noise( p )); p = mtx*p*2.01; f += 0.0625*(-1.0+2.0*noise( p )); return f/0.9375; } float fbm6( vec2 p ) { float f = 0.0; f += 0.500000*noise( p ); p = mtx*p*2.02; f += 0.250000*noise( p ); p = mtx*p*2.03; f += 0.125000*noise( p ); p = mtx*p*2.01; f += 0.062500*noise( p ); p = mtx*p*2.04; f += 0.031250*noise( p ); p = mtx*p*2.01; f += 0.015625*noise( p ); return f/0.96875; } vec2 fbm4_2( vec2 p ) { return vec2( fbm4(p+vec2(1.0)), fbm4(p+vec2(6.2)) ); } vec2 fbm6_2( vec2 p ) { return vec2( fbm6(p+vec2(9.2)), fbm6(p+vec2(5.7)) ); } float func( vec2 q, out vec2 o, out vec2 n ) { q += 0.05*sin(vec2(0.11,0.13)*u_time + length( q )*4.0); q *= 0.7 + 0.2*cos(0.05*u_time); o = 0.5 + 0.5*fbm4_2( q ); o += 0.02*sin(vec2(0.13,0.11)*u_time*length( o )); n = fbm6_2( 4.0*o ); vec2 p = q + 2.0*n + 1.0; float f = 0.5 + 0.5*fbm4( 2.0*p ); f = mix( f, f*f*f*3.5, f*abs(n.x) ); f *= 1.0-0.5*pow( 0.5+0.5*sin(8.0*p.x)*sin(8.0*p.y), 8.0 ); return f; } float funcs( in vec2 q ) { vec2 t1, t2; return func(q,t1,t2); } void main(void ) { vec3 tot = vec3(0.0); #if AA\u0026gt;1 for( int mi=0; mi\u0026lt;AA; mi++ ) for( int ni=0; ni\u0026lt;AA; ni++ ) { // pixel coordinates vec2 of = vec2(float(mi),float(ni)) / float(AA) - 0.5; vec2 q = (2.0*(gl_FragCoord+of)-u_resolution.xy)/u_resolution.y; #else vec2 q = (2.0*gl_FragCoord.xy-u_resolution.xy)/u_resolution.y; #endif vec2 o, n; float f = func(q, o, n); vec3 col = vec3(0.2,0.1,0.4); col = mix( col, vec3(0.3,0.05,0.05), f ); col = mix( col, vec3(0.9,0.9,0.9), dot(n,n) ); col = mix( col, vec3(0.5,0.2,0.2), 0.5*o.y*o.y ); col = mix( col, vec3(0.0,0.2,0.4), 0.5*smoothstep(1.2,1.3,abs(n.y)+abs(n.x)) ); col *= f*2.0; vec2 ex = vec2( 1.0 / u_resolution.x, 0.0 ); vec2 ey = vec2( 0.0, 1.0 / u_resolution.y ); #if AA\u0026gt;1 ex /= float(AA); ey /= float(AA); #endif vec3 nor = normalize( vec3( funcs(q+ex) - f, ex.x, funcs(q+ey) - f ) ); vec3 lig = normalize( vec3( 0.9, -0.2, -0.4 ) ); float dif = clamp( 0.3+0.7*dot( nor, lig ), 0.0, 1.0 ); vec3 lin = vec3(0.85,0.90,0.95)*(nor.y*0.5+0.5); lin += vec3(0.15,0.10,0.05)*dif; col *= lin; col = vec3(1.0)-col; col = col*col; col *= vec3(1.2,1.25,1.2); tot += col; #if AA\u0026gt;1 } tot /= float(AA*AA); #endif vec2 p = gl_FragCoord.xy / u_resolution.xy; tot *= 0.5 + 0.5 * sqrt(16.0*p.x*p.y*(1.0-p.x)*(1.0-p.y)); gl_FragColor = vec4( tot, 1.0 ); } IV. Conclusiones # Se evidenció claramente como el uso de shaders para operaciones como el procesamiento de imágenes, el mapeo de texturas y el texturizado procedimental permite la obtención de resultados altamente eficientes, debido a la capacidad de cómputo de las GPU y los cálculos sobre hardware, comparados con este mismo tipo de procedimientos basados en implementaciones de software. Además, se pudo aprovechar el uso de librerías como p5.treegl que facilitan bastante procedimientos que de otra forma serían más complejos, como la lectura de shaders únicamente con el fragment shader mediante la función readShader de la misma librería, o la manipulación espacial que nos otorga p5.js con el método orbitControl para formas tridimensionales. Similarmente, fue posible evidenciar que existe un gran repertorio de contenido respecto a shaders creado por la comunidad, el cual es sumamemnte útil para aprender y experimentar la manipulación y creación de imágenes digitales.\nV. Referencias # Patricio Gonzalez Vivo. (2015). The Book of Shaders. Getting Started Visual Computing. (2022). Texturing/ Image Processing/ Procedural Texturing. Visual Computing Website Wikipedia contributors. (2022). Texture mapping. Wikipedia. Wikipedia Wikipedia contributors. (2022). Digital image processing. Wikipedia. Wikipedia Nakednous \u0026amp; dangulos @github. (2022). VisualComputing/p5.treegl. p5.treegl WebGLFundamentals. (2015). WebGL Image Processing. WebGLFundamentals Wawan60. (2015). Shadertoy - Magnifier. Source Inigo Quilez. (2013). Shadertoy - Warping - procedural 1. Source Inigo Quilez. (2013). Shadertoy - Warping - procedural 2. Source "},{"id":7,"href":"/VisualComputing-Showcase/workshops/workshop1/moire_patterns/","title":"Moire Patterns","section":"Workshop1","content":" Ramp Aftereffect # a. p5-instance-div markdown let x = 0; let colorp1, colorp2; let increase = 0; function setup() { createCanvas(500, 500); rectMode(CENTER); colorp1 = createColorPicker([32, 162, 32]).position(20, 25); colorp2 = createColorPicker([0, 0, 255]).position(75, 25); slider = createSlider(0, 2, 0, 0.25); slider.position(150, 25); slider.style(\u0026#39;width\u0026#39;, \u0026#39;80px\u0026#39;); } function draw() { background(220); increase = slider.value(); for (let i = 0; i \u0026lt; 400; i += 20) { stroke(colorp2.color()); strokeWeight(4); ellipse(x, 250, i - 380, i - 380); noFill(); stroke(colorp1.color()); strokeWeight(4); ellipse(250, 250, i, i); } if (x \u0026gt; width) { x = 0; } else { x = x + increase; } } "},{"id":8,"href":"/VisualComputing-Showcase/workshops/workshop1/stereokinetic_effect/","title":"Stereokinetic Effect","section":"Workshop1","content":" Stereokinetic Effect # El efecto estereocinético consiste en patrones circulares anidados que rotan sobre una plataforma circular. Estos círculos, al rotar alrededor de algún eje distinto de la línea de visión de quien los observa, permiten extraer la configuración tridimensional del patrón, debido a varias transformaciones ópticas que se producen. En el ejemplo siguiente, podemos observar cómo al hacer rotar varios círculos externos sobre el eje de la plataforma circular, y círculos internos sobre un círculo interno intermedio, en sentido contrario, se puede apreciar una ilusión de un objeto tridimensional, y apreciar profundidad mediante los círculos internos. p5-instance-div markdown const frame_rate = 60; let show_crater_cb; let show_crater = true; let slider_label; function setup() { createCanvas(500, 500); show_crater_cb = createCheckbox(\u0026#39;show crater\u0026#39;, show_crater); show_crater_cb.changed(() =\u0026gt; { show_crater = show_crater_cb.checked(); }); frames_slider = createSlider(0.5, 5, 1, 0.25); frames_slider.position(180, 515); frames_slider.style(\u0026#39;width\u0026#39;, \u0026#39;80px\u0026#39;); slider_label = createSpan(\u0026#39;speed\u0026#39;); slider_label.position(135, 513); frameRate(frame_rate); } function draw() { background(220); let difference = 40; let inner_diameter = 40; const outer_circles = 11; const start = inner_diameter + outer_circles * difference; const end = inner_diameter; noStroke(); let posX = 0, posY = 0; let referenceX = width / 2; let referenceY = height / 2; let t, outer_coeff, inner_coef; for (let diameter = start, index = 0; diameter \u0026gt;= end; diameter -= difference, index++) { fill(index % 2 === 0 ? color(\u0026#39;blue\u0026#39;) : color(\u0026#39;yellow\u0026#39;)); let orientation = index \u0026gt; 6 \u0026amp;\u0026amp; show_crater ? -1 : 1; outer_coeff = orientation * index * difference / 2; t = frameCount * frames_slider.value() / frame_rate; posX = referenceX + outer_coeff * cos(t); posY = referenceY + outer_coeff * sin(t); if (index == 6 \u0026amp;\u0026amp; show_crater) { inner_coef = diameter / 2 + difference; referenceX = posX + inner_coef * cos(t); referenceY = posY + inner_coef * sin(t); } circle(posX, posY, diameter); } } "},{"id":9,"href":"/VisualComputing-Showcase/workshops/workshop1/stroboscopic_artifacts/","title":"Stroboscopic Artifacts","section":"Workshop1","content":" Stroboscopic Artifacts # p5-instance-div markdown let angle = 0; let frames; let colorp1, colorp2, colorp3; let rotation_angle; function setup() { createCanvas(500, 500); frames_slider = createSlider(5, 120, 60, 5); frames_slider.position(180, 40); frames_slider.style(\u0026#39;width\u0026#39;, \u0026#39;80px\u0026#39;); ellipseMode(CENTER); rotation_angle = createP().position(25, 5); frames = createP().position(180, 5); slider = createSlider(0, 360, 0, 5); slider.position(20, 40); slider.style(\u0026#39;width\u0026#39;, \u0026#39;80px\u0026#39;); createP(\u0026#39;Colors:\u0026#39;).position(25, 55).style(\u0026#39;font-size: 15px\u0026#39;); colorp1 = createColorPicker([0, 255, 0]).position(20, 95); colorp2 = createColorPicker([0, 0, 255]).position(20, 135); colorp3 = createColorPicker([255, 0, 0]).position(20, 175); } function draw() { background(200); noStroke(); translate(width/2, height/2); rotate(angle); fill(colorp1.color()); arc(0, 0, width/2, height/2, 0, 2*PI/3); fill(colorp2.color()); arc(0, 0, width/2, height/2, 2*PI/3, 4*PI/3); fill(colorp3.color()); arc(0, 0, width/2, height/2, 4*PI/3, 2*PI); angle += radians(slider.value()); frameRate(frames_slider.value()); rotation_angle.html(\u0026#39;Rotation angle: \u0026#39; + slider.value()); frames.html(\u0026#39;Frame rate: \u0026#39; + frames_slider.value()); } "},{"id":10,"href":"/VisualComputing-Showcase/members/","title":"Members","section":"Introduction","content":" Members # Felipe Rojas Cendales # Estudiante de octavo semestre en la carrera de ingeniería de sistemas, tengo 21 años. Apasionado por el desarrollo de software y la línea de proyectos. Tengo poco conocimiento acerca de animaciones y computación visual, sin embargo, es un tema que me llama mucho la atención.\nEn mi tiempo libre me dedico a mi familia, mis amigos, ver deportes como el automovilismo y practicar otros como una forma de relajación.\nNicolas Arevalo Rodriguez # Soy un estudiante de octava matricula de ingenieria de sistemas y computación, tengo 23 años. Me interesa el desarrollo web, principalmente el front end y espero adquirir la experiencia para implementar animaciones, ilusiones visuales y efectos visualmente atractivos en mis aplicaciones. En mi tiempo libre me dedico a ver series (animadas), jugar en linea con amigos o ver videos/escuchar podcasts sobre temas de mi interés.\nJonathan Lopez Castellanos # Estudiante de octavo semestre en la carrera de ingeniería de sistemas, tengo 21 años. Apasionado por la arquitectura de software y el diseño del mismo.\nEn mi tiempo libre me dedico a proyectos de software propios, ver series en streaming y escuchar buena musiquita.\n"},{"id":11,"href":"/VisualComputing-Showcase/workshops/","title":"Workshops","section":"Introduction","content":" Workshop 1 Workshop 2 Workshop 3 - Juan Felipe Rojas Cendales Workshop 3 - Nicolas Arevalo Rodriguez Workshop 3 - Jonathan López Castellanos "}]